companyName,description,jobName,location,require,salary,timePost
BayOne,"Senior Data engineers with 10+ years of experience. This is a 100% remote role, PST hours (Target West Coast candidates but don't limit your search). Bill rate: $120, PR: $85W2, or $95-$100 C2C. We only need stellar candidates. 3-4 rounds of Interviews (tech screening, including real-time coding). Skill Set: Big Data: Scala, Databricks, Spark SQL, Spark Streaming, Python, Pyspark Cloud : Azure, ADF System Design : API Design, GraphQL, Event-driven architecture, Design Patterns Microservice : Core Java 8 or above, SpringBoot, Azure Functions. Front end ( Nice to Have ) : JavaScripting, React",Senior Data Engineer,"Remote in San Francisco, CA","Senior Data engineers with 10+ years of experience. This is a 100% remote role, PST hours (Target West Coast candidates but don't limit your search). Bill rate: $120, PR: $85W2, or $95-$100 C2C. We only need stellar candidates. 3-4 rounds of Interviews (tech screening, including real-time coding). Skill Set: Big Data: Scala, Databricks, Spark SQL, Spark Streaming, Python, Pyspark Cloud : Azure, ADF System Design : API Design, GraphQL, Event-driven architecture, Design Patterns Microservice : Core Java 8 or above, SpringBoot, Azure Functions. Front end ( Nice to Have ) : JavaScripting, React",$120 an hour,2024-01-14
Samsung Electronics,"Position Summary Contribute to software development and business management by effectively solving business-related software problems through data modeling/analysis/prediction. Role and Responsibilities Contribute to software development and business management by effectively solving business-related software problems through data modeling/analysis/prediction. Establish/support software and infrastructure that continuously grows to strategically analyze big data. Understand and implement software data analysis strategies that meet business strategy. Establish and operate software and infrastructure that can collect/analyze/predict big data (structured/unstructured). Design and operate software structure that can maintain data integrity. Proceed with software data model for business-related decision-making and evaluate data model's consistency. Establish a software security system for data and protect from unauthorized person. Minimum Education & Experience Requirements: Master’s degree in Computer Science, Computer Engineering, Information Technology, a related field, or a foreign equivalent plus 6 months post-baccalaureate experience in job offered or any engineering/IT related job titles. Applicants must have 6 months of experience in the following: (1) statistical analysis and predictive modeling using tools including R, STATA, SAS, and Python; (2) Quantitative Marketing or Operations Research; (3) Web Analytics using Adobe Analytics; (4) data manipulation using SQL; (5) design, execution, and measurement of A/B and multivariate tests; (6) eCommerce and Digital Marketing; (7) building reports and visualizations using Tableau; and (8) software tools including Jira and Confluence. Compensation for this role between $184,662 – $204,662/Year Regular full-time employees (salaried or hourly) have access to benefits including Medical, Dental, Vision, Life Insurance, 401(k), Employee Purchase Program, Tuition Assistance (after 6 months), Paid Time Off, Student Loan Program (after 6 months), Wellness Incentives, and many more. Life @ Samsung - https://www.samsung.com/us/careers/life-at-samsung/ Benefits @ Samsung - https://www.samsung.com/us/careers/benefits/ #LI-DNP Skills and Qualifications At Samsung, we believe that innovation and growth are driven by an inclusive culture and a diverse workforce. We aim to create a global team where everyone belongs and has equal opportunities, inspiring our talent to be their true selves. Together, we are building a better tomorrow for our customers, partners, and communities. Samsung Electronics America, Inc. and its subsidiaries are committed to employing a diverse workforce, and provide Equal Employment Opportunity for all individuals regardless of race, color, religion, gender, age, national origin, marital status, sexual orientation, gender identity, status as a protected veteran, genetic information, status as a qualified individual with a disability, or any other characteristic protected by law. Reasonable Accommodations for Qualified Individuals with Disabilities During the Application Process Samsung Electronics America is committed to providing reasonable accommodations for qualified individuals with disabilities in our job application process. If you have a disability and require a reasonable accommodation in order to participate in the application process, please contact our Reasonable Accommodation Team (855-557-3247) or SEA_Accommodations_Ext@sea.samsung.com for assistance. This number is for accommodation requests only and is not intended for general employment inquiries.","Engineer I, Data Engineering","Mountain View, CA 94043","Position Summary Contribute to software development and business management by effectively solving business-related software problems through data modeling/analysis/prediction. Role and Responsibilities Contribute to software development and business management by effectively solving business-related software problems through data modeling/analysis/prediction. Establish/support software and infrastructure that continuously grows to strategically analyze big data. Understand and implement software data analysis strategies that meet business strategy. Establish and operate software and infrastructure that can collect/analyze/predict big data (structured/unstructured). Design and operate software structure that can maintain data integrity. Proceed with software data model for business-related decision-making and evaluate data model's consistency. Establish a software security system for data and protect from unauthorized person. Minimum Education & Experience Requirements: Master’s degree in Computer Science, Computer Engineering, Information Technology, a related field, or a foreign equivalent plus 6 months post-baccalaureate experience in job offered or any engineering/IT related job titles. Applicants must have 6 months of experience in the following: (1) statistical analysis and predictive modeling using tools including R, STATA, SAS, and Python; (2) Quantitative Marketing or Operations Research; (3) Web Analytics using Adobe Analytics; (4) data manipulation using SQL; (5) design, execution, and measurement of A/B and multivariate tests; (6) eCommerce and Digital Marketing; (7) building reports and visualizations using Tableau; and (8) software tools including Jira and Confluence. Compensation for this role between $184,662 – $204,662/Year Regular full-time employees (salaried or hourly) have access to benefits including Medical, Dental, Vision, Life Insurance, 401(k), Employee Purchase Program, Tuition Assistance (after 6 months), Paid Time Off, Student Loan Program (after 6 months), Wellness Incentives, and many more. Life @ Samsung - https://www.samsung.com/us/careers/life-at-samsung/ Benefits @ Samsung - https://www.samsung.com/us/careers/benefits/ #LI-DNP Skills and Qualifications At Samsung, we believe that innovation and growth are driven by an inclusive culture and a diverse workforce. We aim to create a global team where everyone belongs and has equal opportunities, inspiring our talent to be their true selves. Together, we are building a better tomorrow for our customers, partners, and communities. Samsung Electronics America, Inc. and its subsidiaries are committed to employing a diverse workforce, and provide Equal Employment Opportunity for all individuals regardless of race, color, religion, gender, age, national origin, marital status, sexual orientation, gender identity, status as a protected veteran, genetic information, status as a qualified individual with a disability, or any other characteristic protected by law. Reasonable Accommodations for Qualified Individuals with Disabilities During the Application Process Samsung Electronics America is committed to providing reasonable accommodations for qualified individuals with disabilities in our job application process. If you have a disability and require a reasonable accommodation in order to participate in the application process, please contact our Reasonable Accommodation Team (855-557-3247) or SEA_Accommodations_Ext@sea.samsung.com for assistance. This number is for accommodation requests only and is not intended for general employment inquiries.","$184,662 - $204,662 a year",2024-01-14
BayOne,"Responsibilities Lead and participate in design discussions and meetings Mentor data engineers and analysts Design, automate, build, and launch scalable, efficient and reliable data pipelines into production using Python Perform root cause analysis and resolve production and data issues MINIMUM QUALIFICATIONS: 7+ years experience in data engineering 7+ years experience in custom ETL design, implementation and maintenance Experience building real-time data solutions and processes Advanced skills with Python/Java and SQL are a must Strong computer science fundamentals including data structures and algorithms Experienced in working collaboratively across different teams and departments Strong technical and business communication",Data Engineer/Analyst,"Santa Clara, CA","Responsibilities Lead and participate in design discussions and meetings Mentor data engineers and analysts Design, automate, build, and launch scalable, efficient and reliable data pipelines into production using Python Perform root cause analysis and resolve production and data issues MINIMUM QUALIFICATIONS: 7+ years experience in data engineering 7+ years experience in custom ETL design, implementation and maintenance Experience building real-time data solutions and processes Advanced skills with Python/Java and SQL are a must Strong computer science fundamentals including data structures and algorithms Experienced in working collaboratively across different teams and departments Strong technical and business communication",Competitive,2024-01-14
JLG Industries Inc,"About JLG, an Oshkosh company JLG began in 1969, when our founder, John L. Grove set out to resolve growing safety concerns in the construction industry. Since then we have been committed to understanding the challenges and delivering innovative solutions to the access market. We partner with customers to provide quality equipment, training opportunities and trusted support within the access industry. We are a global company, and our products—including mobile elevating work platforms, telehandlers, utility vehicles and accessories—can be found all over the world. JOB SUMMARY: As a key member in the Data Science team, the Data Engineer will work with a cross functional team to develop and execute the data analytics strategy and apply to Oshkosh Corporation products. Advanced Data Analytics work encompasses the acquisition, processing and machine learning of engineering or service data and application to preventive maintenance and intelligent control of on or off-road vehicles. ESSENTIAL DUTIES AND RESPONSIBILITIES: These duties are not meant to be all-inclusive and other duties may be assigned. Design and develop scalable ETL solutions to deliver data from source systems to analytics platforms (structured and unstructured; batch and streaming). Responsible for testing and validation in order to support the accuracy of data transformations and data verification used with enterprise-wide analytics. Assist in ensuring proper data governance (quality, security, etc.) within the data lake and enterprise data warehouse systems across all business segments. Assist with performance-tuning data processes as well as troubleshooting data processing issues. Collaborate, coordinate, and communicate across disciplines, departments and segments. Develop rapid prototyping and design processes for fast solution delivery to the business. Maintain reference architecture and documentation for the purposes of architectural governance and application roadmap. Assist in educating others on best practices surrounding data work (i.e. data modeling, database design, ETL design, job scheduling and monitoring, etc.). Assist or direct feasibility studies and project estimates (manpower, budget development, and timelines, etc.) on proposed research and development projects. Follow the directions efficiently and provide feedback on the technical hurdles and progresses with clarity and assess the priorities based on business needs. Complete the tasks under the guidance on mutually agreed schedule between the candidate and the supervisor/program manager with minimum mentoring. MINIMUM QUALIFICATIONS: Bachelor’s degree in computer science, data science or a related field with five (5) or more years of working as a data engineer, ETL developer and/or data warehouse DBA. PREFERRED QUALIFICATIONS: Experience with big data tools and architectures, such as Cloudera Hadoop, HDFS, Hive, and Spark. Experience with Azure Dev Ops Experience with Azure cloud services, Databricks, Synapse or similar technologies Experience with developing highly responsive data structures, metadata capture strategies, ontologies, and data dictionaries. Experience with Git (version control) Working knowledge of telematics interfaces and streaming solutions (MQTT, NiFi, Kafka, etc.). Automotive or heavy duty on or off-road vehicle, digital data bus, including Ethernet or Controller Area Network (CAN) experience. Highly organized and detail-oriented, with strong critical thinking, analytical, and problem-solving skills. Ability to handle multiple tasks in a fast-paced environment, both independently and as part of a team. Display excellent interpersonal skills as well as the ability to effectively present information and respond to question from leadership and peers. Strong proficiency with SQL, Bash, and Python (and experience with the Anaconda distribution). Oshkosh is committed to working with and offering reasonable accommodations to job applicants with disabilities. If you need assistance or an accommodation due to a disability for any part of the recruitment process, please contact our reception desk by phone at +1 (920) 502.3009 or our talent acquisition team by email corporatetalentacquisition@oshkoshcorp.com . Oshkosh Corporation is an Equal Opportunity and Affirmative Action Employer. This company will provide equal opportunity to all individuals without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status. Information collected regarding categories as provided by law will in no way affect the decision regarding an employment application. Oshkosh Corporation will not discharge or in any manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with Oshkosh Corporation's legal duty to furnish information. Certain positions with Oshkosh Corporation require access to controlled goods and technologies subject to the International Traffic in Arms Regulations or the Export Administration Regulations. Applicants for these positions may need to be \"U.S. Persons,\" as defined in these regulations. Generally, a \"U.S. Person\" is a U.S. citizen, lawful permanent resident, or an individual who has been admitted as a refugee or granted asylum.",Senior Data Engineer,"Hagerstown, MD","About JLG, an Oshkosh company JLG began in 1969, when our founder, John L. Grove set out to resolve growing safety concerns in the construction industry. Since then we have been committed to understanding the challenges and delivering innovative solutions to the access market. We partner with customers to provide quality equipment, training opportunities and trusted support within the access industry. We are a global company, and our products—including mobile elevating work platforms, telehandlers, utility vehicles and accessories—can be found all over the world. JOB SUMMARY: As a key member in the Data Science team, the Data Engineer will work with a cross functional team to develop and execute the data analytics strategy and apply to Oshkosh Corporation products. Advanced Data Analytics work encompasses the acquisition, processing and machine learning of engineering or service data and application to preventive maintenance and intelligent control of on or off-road vehicles. ESSENTIAL DUTIES AND RESPONSIBILITIES: These duties are not meant to be all-inclusive and other duties may be assigned. Design and develop scalable ETL solutions to deliver data from source systems to analytics platforms (structured and unstructured; batch and streaming). Responsible for testing and validation in order to support the accuracy of data transformations and data verification used with enterprise-wide analytics. Assist in ensuring proper data governance (quality, security, etc.) within the data lake and enterprise data warehouse systems across all business segments. Assist with performance-tuning data processes as well as troubleshooting data processing issues. Collaborate, coordinate, and communicate across disciplines, departments and segments. Develop rapid prototyping and design processes for fast solution delivery to the business. Maintain reference architecture and documentation for the purposes of architectural governance and application roadmap. Assist in educating others on best practices surrounding data work (i.e. data modeling, database design, ETL design, job scheduling and monitoring, etc.). Assist or direct feasibility studies and project estimates (manpower, budget development, and timelines, etc.) on proposed research and development projects. Follow the directions efficiently and provide feedback on the technical hurdles and progresses with clarity and assess the priorities based on business needs. Complete the tasks under the guidance on mutually agreed schedule between the candidate and the supervisor/program manager with minimum mentoring. MINIMUM QUALIFICATIONS: Bachelor’s degree in computer science, data science or a related field with five (5) or more years of working as a data engineer, ETL developer and/or data warehouse DBA. PREFERRED QUALIFICATIONS: Experience with big data tools and architectures, such as Cloudera Hadoop, HDFS, Hive, and Spark. Experience with Azure Dev Ops Experience with Azure cloud services, Databricks, Synapse or similar technologies Experience with developing highly responsive data structures, metadata capture strategies, ontologies, and data dictionaries. Experience with Git (version control) Working knowledge of telematics interfaces and streaming solutions (MQTT, NiFi, Kafka, etc.). Automotive or heavy duty on or off-road vehicle, digital data bus, including Ethernet or Controller Area Network (CAN) experience. Highly organized and detail-oriented, with strong critical thinking, analytical, and problem-solving skills. Ability to handle multiple tasks in a fast-paced environment, both independently and as part of a team. Display excellent interpersonal skills as well as the ability to effectively present information and respond to question from leadership and peers. Strong proficiency with SQL, Bash, and Python (and experience with the Anaconda distribution). Oshkosh is committed to working with and offering reasonable accommodations to job applicants with disabilities. If you need assistance or an accommodation due to a disability for any part of the recruitment process, please contact our reception desk by phone at +1 (920) 502.3009 or our talent acquisition team by email corporatetalentacquisition@oshkoshcorp.com . Oshkosh Corporation is an Equal Opportunity and Affirmative Action Employer. This company will provide equal opportunity to all individuals without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status. Information collected regarding categories as provided by law will in no way affect the decision regarding an employment application. Oshkosh Corporation will not discharge or in any manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with Oshkosh Corporation's legal duty to furnish information. Certain positions with Oshkosh Corporation require access to controlled goods and technologies subject to the International Traffic in Arms Regulations or the Export Administration Regulations. Applicants for these positions may need to be \"U.S. Persons,\" as defined in these regulations. Generally, a \"U.S. Person\" is a U.S. citizen, lawful permanent resident, or an individual who has been admitted as a refugee or granted asylum.",Competitive,2024-01-14
Procter & Gamble,"Job Location Iowa City Job Description P&G is the largest consumer packaged goods company in the world. We have operations in over 75 countries, with 65 trusted brands that improve lives for 5 billion consumers worldwide. This brings many advantages, including the opportunity for our employees to enjoy a diverse and rewarding lifelong career filled with new and exciting challenges. We believe great ideas emerge from the creative connections that happen between our talented employees, and we encourage diverse, multi-functional teams to work together to generate new ideas to address challenges we face. The Data Analyst/Engineer is responsible for the overall design, implementation, and maintenance of systems required for efficient and reliable data processing. This role involves leveraging data analytics tools and digital twin models to extract insights and knowledge from large and complex datasets across the Operations. The Data Analyst/Engineer will utilize various techniques, including statistical analysis, artificial intelligence, machine learning, and data visualization, to support data-driven decision-making processes. Responsibilities: Data Pipeline Development: Design, develop, and maintain data pipelines to ensure the efficient and timely flow of data from various sources into the data infrastructure. Data Integration and Transformation: Perform data integration and transformation tasks to ensure data consistency, quality, and compatibility across different systems and platforms. Data Analysis and Modeling: Analyze large and complex datasets using statistical techniques, identify trends, patterns, and correlations, and develop models to support decision-making processes. AI, Machine Learning, and Predictive Modeling: Apply artificial intelligence and machine learning algorithms to develop predictive models that enable proactive decision-making and improve operational efficiency. Data Visualization and Communication: Create visually appealing and informative data visualizations and reports to effectively communicate insights and findings to stakeholders at all levels of the organization. Continuous Learning and Skill Development: Stay updated with the latest trends, techniques, and tools in data analytics and continuously enhance skills to ensure the adoption of best practices. Business Intelligence: Collaborate with business stakeholders to understand their requirements and provide actionable insights through data analysis, enabling informed decision-making and driving business growth. Collaborative Problem Solving: Work closely with cross-functional teams to identify data-related challenges, propose solutions, and collaborate on implementing data-driven solutions to address business problems. Data Exploration: Conduct exploratory data analysis to discover hidden patterns, trends, and insights that can add value to the organization's operations and decision-making processes. Performance Optimization: Identify opportunities for performance improvement in data processing, storage, and retrieval, and implement optimizations to enhance data infrastructure efficiency. Data Infrastructure and Cloud Technologies: Contribute to the design and development of data infrastructure, leveraging cloud technologies and implementing best practices for data storage, security, and scalability. Job Qualifications Bachelor's or Master's degree in Computer Science, Data Science, or a related field. Strong knowledge of data analysis, data modeling, and statistical techniques. Proficiency in programming languages such as Python, R, SQL, or similar. Experience with data visualization tools such as Tableau, Power BI, or similar. Familiarity with AI and machine learning concepts and frameworks. Excellent problem-solving and critical-thinking skills. Strong communication and presentation skills. Ability to work collaboratively in a team environment. Experience with cloud platforms (e.g., AWS, Azure, GCP) and related technologies is a plus. Compensation for roles at P&G varies depending on a wide array of non-discriminatory factors including but not limited to the specific office location, role, degree/credentials, relevant skill set, and level of relevant experience. At P&G compensation decisions are dependent on the facts and circumstances of each case. Total rewards at P&G include salary + bonus (if applicable) + benefits. Your recruiter may be able to share more about our total rewards offerings and the specific salary range for the relevant location(s) during the hiring process. Just So You Know: We are committed to providing equal opportunities in employment. We value diversity and do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. All will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, disability status, age, sexual orientation, gender identity and expression, marital status, citizenship, HIV/AIDS status or any other legally protected factor. Immigration sponsorship is not available for this position, except in rare situations based on Procter & Gamble's sole discretion. Applicants for U.S. based positions are eligible to work in the U.S. without the need for current or future sponsorship. We do not sponsor for permanent residency. Any exceptions are based on the Company's specific business needs at the time and place of recruitment as well as the particular qualifications of the individual. Procter & Gamble participates in e-verify as required by law. Qualified individuals will not be disadvantaged based on being unemployed. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation. Job Schedule Full time Job Number R000095809 Job Segmentation Recent Grads/Entry Level (Job Segmentation) Starting Pay / Salary Range $85,000.00 - $115,000.00 / year",Data Analyst/Engineer,"Iowa City, IA 52240","Job Location Iowa City Job Description P&G is the largest consumer packaged goods company in the world. We have operations in over 75 countries, with 65 trusted brands that improve lives for 5 billion consumers worldwide. This brings many advantages, including the opportunity for our employees to enjoy a diverse and rewarding lifelong career filled with new and exciting challenges. We believe great ideas emerge from the creative connections that happen between our talented employees, and we encourage diverse, multi-functional teams to work together to generate new ideas to address challenges we face. The Data Analyst/Engineer is responsible for the overall design, implementation, and maintenance of systems required for efficient and reliable data processing. This role involves leveraging data analytics tools and digital twin models to extract insights and knowledge from large and complex datasets across the Operations. The Data Analyst/Engineer will utilize various techniques, including statistical analysis, artificial intelligence, machine learning, and data visualization, to support data-driven decision-making processes. Responsibilities: Data Pipeline Development: Design, develop, and maintain data pipelines to ensure the efficient and timely flow of data from various sources into the data infrastructure. Data Integration and Transformation: Perform data integration and transformation tasks to ensure data consistency, quality, and compatibility across different systems and platforms. Data Analysis and Modeling: Analyze large and complex datasets using statistical techniques, identify trends, patterns, and correlations, and develop models to support decision-making processes. AI, Machine Learning, and Predictive Modeling: Apply artificial intelligence and machine learning algorithms to develop predictive models that enable proactive decision-making and improve operational efficiency. Data Visualization and Communication: Create visually appealing and informative data visualizations and reports to effectively communicate insights and findings to stakeholders at all levels of the organization. Continuous Learning and Skill Development: Stay updated with the latest trends, techniques, and tools in data analytics and continuously enhance skills to ensure the adoption of best practices. Business Intelligence: Collaborate with business stakeholders to understand their requirements and provide actionable insights through data analysis, enabling informed decision-making and driving business growth. Collaborative Problem Solving: Work closely with cross-functional teams to identify data-related challenges, propose solutions, and collaborate on implementing data-driven solutions to address business problems. Data Exploration: Conduct exploratory data analysis to discover hidden patterns, trends, and insights that can add value to the organization's operations and decision-making processes. Performance Optimization: Identify opportunities for performance improvement in data processing, storage, and retrieval, and implement optimizations to enhance data infrastructure efficiency. Data Infrastructure and Cloud Technologies: Contribute to the design and development of data infrastructure, leveraging cloud technologies and implementing best practices for data storage, security, and scalability. Job Qualifications Bachelor's or Master's degree in Computer Science, Data Science, or a related field. Strong knowledge of data analysis, data modeling, and statistical techniques. Proficiency in programming languages such as Python, R, SQL, or similar. Experience with data visualization tools such as Tableau, Power BI, or similar. Familiarity with AI and machine learning concepts and frameworks. Excellent problem-solving and critical-thinking skills. Strong communication and presentation skills. Ability to work collaboratively in a team environment. Experience with cloud platforms (e.g., AWS, Azure, GCP) and related technologies is a plus. Compensation for roles at P&G varies depending on a wide array of non-discriminatory factors including but not limited to the specific office location, role, degree/credentials, relevant skill set, and level of relevant experience. At P&G compensation decisions are dependent on the facts and circumstances of each case. Total rewards at P&G include salary + bonus (if applicable) + benefits. Your recruiter may be able to share more about our total rewards offerings and the specific salary range for the relevant location(s) during the hiring process. Just So You Know: We are committed to providing equal opportunities in employment. We value diversity and do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. All will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, disability status, age, sexual orientation, gender identity and expression, marital status, citizenship, HIV/AIDS status or any other legally protected factor. Immigration sponsorship is not available for this position, except in rare situations based on Procter & Gamble's sole discretion. Applicants for U.S. based positions are eligible to work in the U.S. without the need for current or future sponsorship. We do not sponsor for permanent residency. Any exceptions are based on the Company's specific business needs at the time and place of recruitment as well as the particular qualifications of the individual. Procter & Gamble participates in e-verify as required by law. Qualified individuals will not be disadvantaged based on being unemployed. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation. Job Schedule Full time Job Number R000095809 Job Segmentation Recent Grads/Entry Level (Job Segmentation) Starting Pay / Salary Range $85,000.00 - $115,000.00 / year","$85,000 - $115,000 a year",2024-01-14
AT&T,"We are so glad you are interested in joining AT&T. Education: Bachelor's Degree: Computer Engineering, Bachelor's Degree: Computer Science, Bachelor's Degree: Data Science, Bachelor's Degree: Machine Learning Engineer Nanodegree, Bachelor's Degree: Mathematics, Bachelor's Degree: Statistics Job Description: At AT&T we reimagine the communications and technologies that connect the world. Our Consumer Technology eXperience team is delivering innovative and reliable technology solutions to power differentiated, simplified customer experiences. Bring your bold ideas and join us on our mission to be the best connectivity provider through 5G and Fiber. When you step into a career with AT&T, you won’t just imagine the future- you’ll create it. About the Team: The Virtual Assistant team is committed to delivering an intelligent, personalized, and proactive virtual assistant that boosts call deflection and generates incremental digital revenue in the customer service domain. Our mission is to delight customers with personalized experiences powered by cutting-edge digital tools, ensuring quick and accurate issue resolution while reducing the burden on live support agents. We leverage innovative technologies such as AI and Machine learning, that amplify our best opportunities for success, aiming to revolutionize customer service experiences while driving efficiency and growth for our organization. About the Job: We are seeking a Senior data engineer who enjoys digging into metrics and iterating quickly to find ways to scale sustainably. You will handle everything ranging from front-end to back-end, building solutions in a fast-paced and collaborative process. You will have an integral role in technical decisions that define our product and influence the customer experience. You’ll be part of a collaborative and highly autonomous work environment where each member has a defined role with clear expectations and the freedom to pursue projects, they find interesting. You will be working closely with Machine Learning Engineers on the team. What you’ll do: Work with the latest AI technologies, tools, and techniques, including ChatGPT, LLama2, RAG, and advanced fine-tuning methods Drive GenAI use cases across various domains such as Customer Service (Chat and IVR), Sales, Marketing, and Developer Tools Maintain and upgrade software to ensure it meets the latest performance and security standards Oversee the technical aspects of projects, ensuring high-quality delivery and alignment with business goals Collaborate with cross-functional teams to define, design, and ship new features while ensuring consistency with client requirements and expectations Serve as a technical expert and adviser for engineering teams, providing technical leadership in decision-making and problem-solving efforts Design and build large and complex data sets, from spurious sources while thinking strategically about uses of data and how data use interacts with data design Design and implement statistical data quality procedures around new data sources Perform data studies and data discovery around new data sources or new uses for existing data sources Implement any software required for accessing and handling data appropriately. Implement and hand off data checking and updating procedures to teams Perform statistical analyses with existing data sets in statistical packages Visualize and report data findings creatively in a variety of visual formats that appropriately provides insights to the organization Research and implement software and hardware related to mobile technology What you’ll need: Required Qualifications: 8+ years’ experience in Data Engineering Desired Qualifications: Bachelor’s degree in Computer Engineering/ Computer Science, Statistics, Math, Data Science or Machine Learning nano degree or related field preferred Data Science / Machine Learning certification(s) Strong experience in Python and well-versed in data analytical frameworks and structured Data modelling Analytics experience Data warehousing experience Root cause analytics experience Requirements documentation experience Strong experience in Predictive modeling, NLP, Supervised, unsupervised, Tree-based modeling, neural networks, analytics tools and frameworks Proven experience in Snowflake, Hadoop, and fluent with SQL MySQL, T-SQL and/or PL/SQL) and data analysis techniques. Proven experience with Jupyter or equivalent tools Proven experience in Cloud technologies like Azure, AWS, and Deep data platforms Proven experience in software development methodologies, Agile Proven experience building complex web systems Excellent organizational, leadership and communication skills Strong financial acumen and analysis experience to determine business impacts for the application of the statistical solution Our Principal-Software Engineers earn between $141,300 - $237,400. Not to mention all the other amazing rewards that working at AT&T offers. Individual starting salary within this range may depend on geography, experience, expertise, and education/training. Joining our team comes with amazing perks and benefits: Medical/Dental/Vision coverage 401(k) plan Tuition reimbursement program Paid Time Off and Holidays (based on date of hire, at least 23 days of vacation each year and 9 company-designated holidays) Paid Parental Leave Paid Caregiver Leave Additional sick leave beyond what state and local law require may be available but is unprotected Adoption Reimbursement Disability Benefits (short term and long term) Life and Accidental Death Insurance Supplemental benefit programs: critical illness/accident hospital indemnity/group legal Employee Assistance Programs (EAP) Extensive employee wellness programs Employee discounts up to 50% off on eligible AT&T mobility plans and accessories, AT&T internet (and fiber where available) and AT&T phone AT&T is leading the way to the future – for customers, businesses and the industry. We're developing new technologies to make it easier for our customers to stay connected to their world. Together, we're building a world of possibilities and an amazing place to work and grow. Team up with industry innovators every time you walk into work, creating the world you always imagined. Ready to #transformdigital with us? Apply now! Weekly Hours: 40 Time Type: Regular Location: Plano, Texas Salary Range: $141,300.00 - $237,400.00 It is the policy of AT&T to provide equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, AT&T will provide reasonable accommodations for qualified individuals with disabilities.",Data Engineer,"Atlanta, GA","We are so glad you are interested in joining AT&T. Education: Bachelor's Degree: Computer Engineering, Bachelor's Degree: Computer Science, Bachelor's Degree: Data Science, Bachelor's Degree: Machine Learning Engineer Nanodegree, Bachelor's Degree: Mathematics, Bachelor's Degree: Statistics Job Description: At AT&T we reimagine the communications and technologies that connect the world. Our Consumer Technology eXperience team is delivering innovative and reliable technology solutions to power differentiated, simplified customer experiences. Bring your bold ideas and join us on our mission to be the best connectivity provider through 5G and Fiber. When you step into a career with AT&T, you won’t just imagine the future- you’ll create it. About the Team: The Virtual Assistant team is committed to delivering an intelligent, personalized, and proactive virtual assistant that boosts call deflection and generates incremental digital revenue in the customer service domain. Our mission is to delight customers with personalized experiences powered by cutting-edge digital tools, ensuring quick and accurate issue resolution while reducing the burden on live support agents. We leverage innovative technologies such as AI and Machine learning, that amplify our best opportunities for success, aiming to revolutionize customer service experiences while driving efficiency and growth for our organization. About the Job: We are seeking a Senior data engineer who enjoys digging into metrics and iterating quickly to find ways to scale sustainably. You will handle everything ranging from front-end to back-end, building solutions in a fast-paced and collaborative process. You will have an integral role in technical decisions that define our product and influence the customer experience. You’ll be part of a collaborative and highly autonomous work environment where each member has a defined role with clear expectations and the freedom to pursue projects, they find interesting. You will be working closely with Machine Learning Engineers on the team. What you’ll do: Work with the latest AI technologies, tools, and techniques, including ChatGPT, LLama2, RAG, and advanced fine-tuning methods Drive GenAI use cases across various domains such as Customer Service (Chat and IVR), Sales, Marketing, and Developer Tools Maintain and upgrade software to ensure it meets the latest performance and security standards Oversee the technical aspects of projects, ensuring high-quality delivery and alignment with business goals Collaborate with cross-functional teams to define, design, and ship new features while ensuring consistency with client requirements and expectations Serve as a technical expert and adviser for engineering teams, providing technical leadership in decision-making and problem-solving efforts Design and build large and complex data sets, from spurious sources while thinking strategically about uses of data and how data use interacts with data design Design and implement statistical data quality procedures around new data sources Perform data studies and data discovery around new data sources or new uses for existing data sources Implement any software required for accessing and handling data appropriately. Implement and hand off data checking and updating procedures to teams Perform statistical analyses with existing data sets in statistical packages Visualize and report data findings creatively in a variety of visual formats that appropriately provides insights to the organization Research and implement software and hardware related to mobile technology What you’ll need: Required Qualifications: 8+ years’ experience in Data Engineering Desired Qualifications: Bachelor’s degree in Computer Engineering/ Computer Science, Statistics, Math, Data Science or Machine Learning nano degree or related field preferred Data Science / Machine Learning certification(s) Strong experience in Python and well-versed in data analytical frameworks and structured Data modelling Analytics experience Data warehousing experience Root cause analytics experience Requirements documentation experience Strong experience in Predictive modeling, NLP, Supervised, unsupervised, Tree-based modeling, neural networks, analytics tools and frameworks Proven experience in Snowflake, Hadoop, and fluent with SQL MySQL, T-SQL and/or PL/SQL) and data analysis techniques. Proven experience with Jupyter or equivalent tools Proven experience in Cloud technologies like Azure, AWS, and Deep data platforms Proven experience in software development methodologies, Agile Proven experience building complex web systems Excellent organizational, leadership and communication skills Strong financial acumen and analysis experience to determine business impacts for the application of the statistical solution Our Principal-Software Engineers earn between $141,300 - $237,400. Not to mention all the other amazing rewards that working at AT&T offers. Individual starting salary within this range may depend on geography, experience, expertise, and education/training. Joining our team comes with amazing perks and benefits: Medical/Dental/Vision coverage 401(k) plan Tuition reimbursement program Paid Time Off and Holidays (based on date of hire, at least 23 days of vacation each year and 9 company-designated holidays) Paid Parental Leave Paid Caregiver Leave Additional sick leave beyond what state and local law require may be available but is unprotected Adoption Reimbursement Disability Benefits (short term and long term) Life and Accidental Death Insurance Supplemental benefit programs: critical illness/accident hospital indemnity/group legal Employee Assistance Programs (EAP) Extensive employee wellness programs Employee discounts up to 50% off on eligible AT&T mobility plans and accessories, AT&T internet (and fiber where available) and AT&T phone AT&T is leading the way to the future – for customers, businesses and the industry. We're developing new technologies to make it easier for our customers to stay connected to their world. Together, we're building a world of possibilities and an amazing place to work and grow. Team up with industry innovators every time you walk into work, creating the world you always imagined. Ready to #transformdigital with us? Apply now! Weekly Hours: 40 Time Type: Regular Location: Plano, Texas Salary Range: $141,300.00 - $237,400.00 It is the policy of AT&T to provide equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, AT&T will provide reasonable accommodations for qualified individuals with disabilities.","$141,300 - $237,400 a year",2024-01-14
American Airlines,"Location: DFW Headquarters Building 7 (DFW-SV07) Additional Locations: None Requisition ID: 70826 Intro Are you ready to explore a world of possibilities, both at work and during your time off? Join our American Airlines family, and you’ll travel the world, grow your expertise and become the best version of you. As you embark on a new journey, you’ll tackle challenges with flexibility and grace, learning new skills and advancing your career while having the time of your life. Feel free to enrich both your personal and work life and hop on board! Why you'll love this job You will help enable data engineering solutions at AA You will be part of a team that innovates. This role is a part of the Data Engineering and Analytics team within our Technology group. You’ll bring your data engineering, collaboration and analytics skills to help cultivate a data driven culture by designing and delivering analytics solutions and making data analytics easier and more effective for American Airlines. What you'll do As noted above, this list is intended to reflect the current job but there may be additional essential functions (and certainly non-essential job functions) that are not referenced. Management will modify the job or require other tasks be performed whenever it is deemed appropriate to do so, observing, of course, any legal obligations including any collective bargaining obligations. Work closely with source data application teams and product owners to design, implement and support analytics solutions that provide insights to make better decisions Implement data migration and data engineering solutions using Azure products and services: (Azure Data Lake Storage, Azure Data Factory, Azure Functions, Event Hub, Azure Stream Analytics, Azure Databricks, etc.) and traditional data warehouse tools. Perform multiple aspects involved in the development lifecycle – design, cloud engineering (Infrastructure, network, security, and administration), ingestion, preparation, data modeling, testing, CICD pipelines, performance tuning, deployments, consumption, BI, alerting, prod support. Provide technical leadership and collaborate within a team environment as well as work independently. Be a part of a DevOps team that completely owns and supports their product Implement batch and streaming data pipelines using cloud technologies Leads development of coding standards, best practices and privacy and security guidelines. Mentors others on technical and domain skills to create multi-functional teams All you'll need for success Minimum Qualifications- Education & Prior Job Experience Bachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training 3 years software solution development using agile, DevOps, operating in a product model that includes designing, developing, and implementing large-scale applications or data engineering solutions 3 years data analytics experience using SQL 2 years of cloud development and data lake experience (prefer Microsoft Azure) including Azure EventHub, Azure Data Factory, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Power BI. Combination of Development, Administration & Support experience in several of the following tools/platforms required: Scripting: Python, Spark, Unix, SQL Data Platforms: Teradata, Cassandra, MongoDB, Oracle, SQL Server, ADLS, Snowflake Azure Data Explorer. Administration skills a plus Azure Cloud Technologies: Azure Data Factory, Azure Databricks, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Azure Functions CI/CD: GitHub, Jenkins, Azure DevOps, Terraform BI Analytics Tool Stack - Cognos, Tableau, Power BI, Alteryx, Denodo, and Grafana Data Warehousing: DataStage, Informatica Data Governance and Privacy: Informatica Axon and EDC, BigID Preferred Qualifications- Education & Prior Job Experience 5+ years software solution development using agile, dev ops, product model that includes designing, developing, and implementing large-scale applications or data engineering solutions. 5+ years data analytics experience using SQL 3+ years full-stack development experience, preferably in Azure 3+ years of cloud development and data lake experience (prefer Microsoft Azure) including Azure EventHub, Azure Data Factory, Azure Functions, ADX, ASA, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Power BI. Airline Industry Experience Skills, Licenses & Certifications Expertise with the Azure Technology stack for data management, data ingestion, capture, processing, curation and creating consumption layers. Expertise in providing practical direction within the Azure Native cloud services. Azure Development Track Certification (preferred) Spark Certification (preferred) What you'll get Feel free to take advantage of all that American Airlines has to offer: Travel Perks: Ready to explore the world? You, your family and your friends can reach 365 destinations on more than 6,800 daily flights across our global network. Health Benefits: On day one, you’ll have access to your health, dental, prescription and vision benefits to help you stay well. And that’s just the start, we also offer virtual doctor visits, flexible spending accounts and more. Wellness Programs: We want you to be the best version of yourself – that’s why our wellness programs provide you with all the right tools, resources and support you need. 401(k) Program: Available upon hire and, depending on the workgroup, employer contributions to your 401(k) program are available after one year. Additional Benefits: Other great benefits include our Employee Assistance Program, pet insurance and discounts on hotels, cars, cruises and more Feel free to be yourself at American From the team members we hire to the customers we serve, inclusion and diversity are the foundation of the dynamic workforce at American Airlines. Our 20+ Employee Business Resource Groups are focused on connecting our team members to our customers, suppliers, communities and shareholders, helping team members reach their full potential and creating an inclusive work environment to meet and exceed the needs of our diverse world. Are you ready to feel a tremendous sense of pride and satisfaction as you do your part to keep the largest airline in the world running smoothly as we care for people on life’s journey? Feel free to be yourself at American. Additional Locations: None Requisition ID: 70826","Senior Data Engineer, IT Analytics","Dallas, TX 75219 (Downtown area)","Location: DFW Headquarters Building 7 (DFW-SV07) Additional Locations: None Requisition ID: 70826 Intro Are you ready to explore a world of possibilities, both at work and during your time off? Join our American Airlines family, and you’ll travel the world, grow your expertise and become the best version of you. As you embark on a new journey, you’ll tackle challenges with flexibility and grace, learning new skills and advancing your career while having the time of your life. Feel free to enrich both your personal and work life and hop on board! Why you'll love this job You will help enable data engineering solutions at AA You will be part of a team that innovates. This role is a part of the Data Engineering and Analytics team within our Technology group. You’ll bring your data engineering, collaboration and analytics skills to help cultivate a data driven culture by designing and delivering analytics solutions and making data analytics easier and more effective for American Airlines. What you'll do As noted above, this list is intended to reflect the current job but there may be additional essential functions (and certainly non-essential job functions) that are not referenced. Management will modify the job or require other tasks be performed whenever it is deemed appropriate to do so, observing, of course, any legal obligations including any collective bargaining obligations. Work closely with source data application teams and product owners to design, implement and support analytics solutions that provide insights to make better decisions Implement data migration and data engineering solutions using Azure products and services: (Azure Data Lake Storage, Azure Data Factory, Azure Functions, Event Hub, Azure Stream Analytics, Azure Databricks, etc.) and traditional data warehouse tools. Perform multiple aspects involved in the development lifecycle – design, cloud engineering (Infrastructure, network, security, and administration), ingestion, preparation, data modeling, testing, CICD pipelines, performance tuning, deployments, consumption, BI, alerting, prod support. Provide technical leadership and collaborate within a team environment as well as work independently. Be a part of a DevOps team that completely owns and supports their product Implement batch and streaming data pipelines using cloud technologies Leads development of coding standards, best practices and privacy and security guidelines. Mentors others on technical and domain skills to create multi-functional teams All you'll need for success Minimum Qualifications- Education & Prior Job Experience Bachelor's degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training 3 years software solution development using agile, DevOps, operating in a product model that includes designing, developing, and implementing large-scale applications or data engineering solutions 3 years data analytics experience using SQL 2 years of cloud development and data lake experience (prefer Microsoft Azure) including Azure EventHub, Azure Data Factory, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Power BI. Combination of Development, Administration & Support experience in several of the following tools/platforms required: Scripting: Python, Spark, Unix, SQL Data Platforms: Teradata, Cassandra, MongoDB, Oracle, SQL Server, ADLS, Snowflake Azure Data Explorer. Administration skills a plus Azure Cloud Technologies: Azure Data Factory, Azure Databricks, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Azure Functions CI/CD: GitHub, Jenkins, Azure DevOps, Terraform BI Analytics Tool Stack - Cognos, Tableau, Power BI, Alteryx, Denodo, and Grafana Data Warehousing: DataStage, Informatica Data Governance and Privacy: Informatica Axon and EDC, BigID Preferred Qualifications- Education & Prior Job Experience 5+ years software solution development using agile, dev ops, product model that includes designing, developing, and implementing large-scale applications or data engineering solutions. 5+ years data analytics experience using SQL 3+ years full-stack development experience, preferably in Azure 3+ years of cloud development and data lake experience (prefer Microsoft Azure) including Azure EventHub, Azure Data Factory, Azure Functions, ADX, ASA, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Data Lake, Azure Power Apps and Power BI. Airline Industry Experience Skills, Licenses & Certifications Expertise with the Azure Technology stack for data management, data ingestion, capture, processing, curation and creating consumption layers. Expertise in providing practical direction within the Azure Native cloud services. Azure Development Track Certification (preferred) Spark Certification (preferred) What you'll get Feel free to take advantage of all that American Airlines has to offer: Travel Perks: Ready to explore the world? You, your family and your friends can reach 365 destinations on more than 6,800 daily flights across our global network. Health Benefits: On day one, you’ll have access to your health, dental, prescription and vision benefits to help you stay well. And that’s just the start, we also offer virtual doctor visits, flexible spending accounts and more. Wellness Programs: We want you to be the best version of yourself – that’s why our wellness programs provide you with all the right tools, resources and support you need. 401(k) Program: Available upon hire and, depending on the workgroup, employer contributions to your 401(k) program are available after one year. Additional Benefits: Other great benefits include our Employee Assistance Program, pet insurance and discounts on hotels, cars, cruises and more Feel free to be yourself at American From the team members we hire to the customers we serve, inclusion and diversity are the foundation of the dynamic workforce at American Airlines. Our 20+ Employee Business Resource Groups are focused on connecting our team members to our customers, suppliers, communities and shareholders, helping team members reach their full potential and creating an inclusive work environment to meet and exceed the needs of our diverse world. Are you ready to feel a tremendous sense of pride and satisfaction as you do your part to keep the largest airline in the world running smoothly as we care for people on life’s journey? Feel free to be yourself at American. Additional Locations: None Requisition ID: 70826",Competitive,2024-01-14
Google,"The application window will be open until at least January 18, 2024. This opportunity will remain online based on business needs which may be before or after the specified date. Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Atlanta, GA, USA; Boulder, CO, USA. Minimum qualifications: Bachelor’s degree or equivalent practical experience. Experience designing data models, data warehouses, and using SQL and NoSQL database management systems along with data processing using traditional and distributed systems (e.g., Hadoop, Spark, Dataflow, Airflow). Experience in one or more object oriented programming languages (Java, C++, Python, etc.). Preferred qualifications: Experience writing and maintaining ETLs which operate on a variety of structured and unstructured sources. Experience with Unix or GNU/Linux systems. Experience in large-scale distributed data processing. Experience with modeling business processes or real-world data sources. Excellent written communication, organizational, and analytical skills. About the job At gTech’s Users and Products team (gUP), our mission is to help users get the most out of Google. We represent Google's users and many of our partners globally, sharing insights with the larger Google organization to enable exceptional customer and product experiences. gUP builds innovative solutions that take user experience and engagement with Google to the next level, supporting users across products, countries, cultures, incomes, and identities. We advocate for users through partnerships with product areas at Google (and some Alphabet businesses), supporting Google’s consumer products ecosystem and enabling numerous launches for Google’s consumer products each year. In this role, you will help us build data-powered decision making tools. Google creates products and services that make the world a better place, and gTech’s role is to help bring them to life. Our teams of trusted advisors support customers globally. Our solutions are rooted in our technical skill, product expertise, and a thorough understanding of our customers’ complex needs. Whether the answer is a bespoke solution to solve a unique problem, or a new tool that can scale across Google, everything we do aims to ensure our customers benefit from the full potential of Google products. To learn more about gTech, check out our video. The US base salary range for this full-time position is $93,500-$135,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google. Responsibilities Design, develop and support data pipelines, warehouses and reporting systems to solve business operations, users, and products' problems. Create Extract, Transform, and Load (ETLs) and reporting systems for new data using a variety of traditional as well as large-scale distributed data systems. Collaborate and influence Users and Products stakeholders and support engineers to ensure our data infrastructure meets constantly evolving requirements. Work closely with data scientists to productionize various statistical and machine learning models using data processing pipelines. Recommend improvements and implement modifications to existing data models and ETL pipelines. Write and review technical documents including design, development, and revision documents. Drive innovation with new analytical tools to gain even more insight from data. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form.","Data Engineer II, gTech","Atlanta, GA","The application window will be open until at least January 18, 2024. This opportunity will remain online based on business needs which may be before or after the specified date. Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Atlanta, GA, USA; Boulder, CO, USA. Minimum qualifications: Bachelor’s degree or equivalent practical experience. Experience designing data models, data warehouses, and using SQL and NoSQL database management systems along with data processing using traditional and distributed systems (e.g., Hadoop, Spark, Dataflow, Airflow). Experience in one or more object oriented programming languages (Java, C++, Python, etc.). Preferred qualifications: Experience writing and maintaining ETLs which operate on a variety of structured and unstructured sources. Experience with Unix or GNU/Linux systems. Experience in large-scale distributed data processing. Experience with modeling business processes or real-world data sources. Excellent written communication, organizational, and analytical skills. About the job At gTech’s Users and Products team (gUP), our mission is to help users get the most out of Google. We represent Google's users and many of our partners globally, sharing insights with the larger Google organization to enable exceptional customer and product experiences. gUP builds innovative solutions that take user experience and engagement with Google to the next level, supporting users across products, countries, cultures, incomes, and identities. We advocate for users through partnerships with product areas at Google (and some Alphabet businesses), supporting Google’s consumer products ecosystem and enabling numerous launches for Google’s consumer products each year. In this role, you will help us build data-powered decision making tools. Google creates products and services that make the world a better place, and gTech’s role is to help bring them to life. Our teams of trusted advisors support customers globally. Our solutions are rooted in our technical skill, product expertise, and a thorough understanding of our customers’ complex needs. Whether the answer is a bespoke solution to solve a unique problem, or a new tool that can scale across Google, everything we do aims to ensure our customers benefit from the full potential of Google products. To learn more about gTech, check out our video. The US base salary range for this full-time position is $93,500-$135,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google. Responsibilities Design, develop and support data pipelines, warehouses and reporting systems to solve business operations, users, and products' problems. Create Extract, Transform, and Load (ETLs) and reporting systems for new data using a variety of traditional as well as large-scale distributed data systems. Collaborate and influence Users and Products stakeholders and support engineers to ensure our data infrastructure meets constantly evolving requirements. Work closely with data scientists to productionize various statistical and machine learning models using data processing pipelines. Recommend improvements and implement modifications to existing data models and ETL pipelines. Write and review technical documents including design, development, and revision documents. Drive innovation with new analytical tools to gain even more insight from data. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form.",Competitive,2024-01-14
HealthPartners/GHI,"JOB DESCRIPTION SENIOR DATA ENGINEER (Multiple Positions) – BLOOMINGTON, MN. Responsible for building, managing, and optimizing data pipelines that facilitate data movement in service of these goals by implementing and testing methods or build systems that improve data reliability and quality. Duties include: work with stakeholders, data scientists and analysts to frame problems, clean and integrate data, and determine best way to provision data on demand; collaborate with developers to design technology solutions that achieve measurable results at scale; help design and develop scalable, efficient data pipeline processes to handle data ingestion, cleansing, transformation, integration, and validation required to provide access to prepared data sets for analysts and data scientists; utilize development best practices including technical design reviews, implementing test plans, monitoring/alerting, peer code reviews, and documentation; collaborate with cross functional team to resolve data quality and operational issues and ensure timely delivery of products; incorporate core data management competencies including data governance, data security and data quality; participate in requirements gathering sessions to distill technical requirement from business requests; and, identify, design, and implement internal process improvements. Reqs. Master’s degree in Computer Science, Engineering, or closely related technical field, or, Bachelor’s degree in Computer Science, Engineering, or closely related technical field plus 2 years’ experience with hands-on data engineering and SQL & Python. Reqs. education, experience, or training in: data modeling techniques such as star-/snowflake-schema, de-normalized data modeling, and 3NF; data formats such as parquet, avro, delta, csv, and json; data processing techniques such as full-batch processing, time-based partitioning, distributed and real-time processing; data profiling; and, CI/CD and version control tools. May permit telecommuting. Apply at www.healthpartners.com/careers. Reference Job ID 10031. ABOUT US At HealthPartners we believe in the power of good – good deeds and good people working together. As part of our team, you’ll find an inclusive environment that encourages new ways of thinking, celebrates differences, and recognizes hard work. We’re a nonprofit, integrated health care organization, providing health insurance in six states and high-quality care at more than 90 locations, including hospitals and clinics in Minnesota and Wisconsin. We bring together research and education through HealthPartners Institute, training medical professionals across the region and conducting innovative research that improve lives around the world. At HealthPartners, everyone is welcome, included and valued. We’re working together to increase diversity and inclusion in our workplace, advance health equity in care and coverage, and partner with the community as advocates for change. Join us and become a partner for good, helping to improve the health and well-being of our patients, members and the communities we serve. We are an Equal Opportunity Employer and do not discriminate against any employee or applicant because of race, color, sex, age, national origin, religion, sexual orientation, gender identify, status as a veteran and basis of disability or any other federal, state or local protected class. JOB INFO Job Identification 100301 Posting Date 01/08/2024, 04:21 PM Locations 8170 33rd Ave S - Bloomington Work Schedule Monday-Friday; Core business hours Hours Per Week/FTE 40 hrs weekly / 1.0 FTE Job Shift Day Position Type Full-time regular Job Category Information Technology Department Health Informatics",Senior Data Engineer,"Bloomington, MN","JOB DESCRIPTION SENIOR DATA ENGINEER (Multiple Positions) – BLOOMINGTON, MN. Responsible for building, managing, and optimizing data pipelines that facilitate data movement in service of these goals by implementing and testing methods or build systems that improve data reliability and quality. Duties include: work with stakeholders, data scientists and analysts to frame problems, clean and integrate data, and determine best way to provision data on demand; collaborate with developers to design technology solutions that achieve measurable results at scale; help design and develop scalable, efficient data pipeline processes to handle data ingestion, cleansing, transformation, integration, and validation required to provide access to prepared data sets for analysts and data scientists; utilize development best practices including technical design reviews, implementing test plans, monitoring/alerting, peer code reviews, and documentation; collaborate with cross functional team to resolve data quality and operational issues and ensure timely delivery of products; incorporate core data management competencies including data governance, data security and data quality; participate in requirements gathering sessions to distill technical requirement from business requests; and, identify, design, and implement internal process improvements. Reqs. Master’s degree in Computer Science, Engineering, or closely related technical field, or, Bachelor’s degree in Computer Science, Engineering, or closely related technical field plus 2 years’ experience with hands-on data engineering and SQL & Python. Reqs. education, experience, or training in: data modeling techniques such as star-/snowflake-schema, de-normalized data modeling, and 3NF; data formats such as parquet, avro, delta, csv, and json; data processing techniques such as full-batch processing, time-based partitioning, distributed and real-time processing; data profiling; and, CI/CD and version control tools. May permit telecommuting. Apply at www.healthpartners.com/careers. Reference Job ID 10031. ABOUT US At HealthPartners we believe in the power of good – good deeds and good people working together. As part of our team, you’ll find an inclusive environment that encourages new ways of thinking, celebrates differences, and recognizes hard work. We’re a nonprofit, integrated health care organization, providing health insurance in six states and high-quality care at more than 90 locations, including hospitals and clinics in Minnesota and Wisconsin. We bring together research and education through HealthPartners Institute, training medical professionals across the region and conducting innovative research that improve lives around the world. At HealthPartners, everyone is welcome, included and valued. We’re working together to increase diversity and inclusion in our workplace, advance health equity in care and coverage, and partner with the community as advocates for change. Join us and become a partner for good, helping to improve the health and well-being of our patients, members and the communities we serve. We are an Equal Opportunity Employer and do not discriminate against any employee or applicant because of race, color, sex, age, national origin, religion, sexual orientation, gender identify, status as a veteran and basis of disability or any other federal, state or local protected class. JOB INFO Job Identification 100301 Posting Date 01/08/2024, 04:21 PM Locations 8170 33rd Ave S - Bloomington Work Schedule Monday-Friday; Core business hours Hours Per Week/FTE 40 hrs weekly / 1.0 FTE Job Shift Day Position Type Full-time regular Job Category Information Technology Department Health Informatics",Competitive,2024-01-14
The Motley Fool,"The Motley Fool is looking for a highly skilled Freelance Data Engineer to join our team on an independent contract basis, 40 hours per week for 6-12 months. This is a mid to senior level position and requires 4-5+ years of relevant experience. This role is flexible and 100% remote, but candidates MUST reside in the United States to be considered. Who are we? We are The Motley Fool, a purpose-driven financial information and services firm with more than 30 years of experience focused on making the world smarter, happier, and richer. But what does that even mean?! It means we're helping Fools (always with a capital \"F\") demystify the world of finance, beat the stock market, and achieve personal wealth and happiness through our products and services. The Motley Fool is firmly committed to diversity, inclusion, and equity. We are a motley group of overachievers that have built a culture of trust founded on Foolishness, fun, and a commitment to making the world smarter, happier and richer. However you identify or whatever winding road has led you to us, please don't hesitate to apply if the description above leaves you thinking, \"Hey! I could do that!\" What does this team do? The Data Engineering team at The Motley Fool creates data pipelines to wrangle data from around the Fool. We collaborate with everyone - from third party vendors to stakeholders to build easily consumable data structures for reporting and business insights. While working closely with our business analysts and machine learning specialists, we serve the data needs of all The Motley Fool Teams! What would you do in this role? As a Freelance Data Engineer, you will be responsible for expanding and optimizing data, the data pipeline architecture, the data flow, and collection for cross-functional teams. You are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. You will help to guide and support our software developers, database architects, data analysts, and data scientists on business initiatives while ensuring optimal data delivery architecture is consistent. Whether it's working on a solo project or with the team, you are self-directed and comfortable supporting the data needs of multiple teams, systems, and products. But What Would You Actually Do in this role? Leverage data assets to meet mission needs, ensuring consistent data quality, establishing data standards and governance Work in an agile, collaborative environment, partnering with client stakeholders, to develop and improve mission-based solutions Monitor cloud-based systems and components for availability, performance, reliability, security and efficiency Create and configure appropriate cloud resources to meet the needs of the end users. Strong, proven problem-solving skills and a proven ability to apply critical/analytical thinking to deliver sustainable and creative solutions to complex requirements. As needed, document topology, processes, and solution architecture. Assist with the training and enablement of data consumers. Share your passion for staying on top of tech trends, experimenting with and learning new technologies Required Experience: 3+ years data modeling experience 3+ years with Python 2+ years of experience with AWS Services Experience with AWS Glue and AWS Athena Experience with cloud data storage and compute components including lambda functions, EC2s, containers. Ability to work independently, and deliver results and drive projects with minimal supervision Strong ability to communicate blockers and issues to management for escalation and timely resolution Strong team player, with desire to learn new skills and broaden experience Experience working with complex data sets Preferred Qualifications: 2+ years with Snowflake 2+ years with Apache Spark 1+ years with Apache Airflow Experience working with Financial data Experience investing and/or using The Motley Fool's service offerings Hourly Pay Range $85—$105 USD By applying on this site, you acknowledge that The Motley Fool will be collecting the personal data you provide for our recruiting purposes. Please see our Applicant Privacy Notice for additional information about how we process, transfer, and store your data, including where that data is stored, and about any additional privacy rights you may have based on your jurisdiction.",Freelance Data Engineer,Remote,"The Motley Fool is looking for a highly skilled Freelance Data Engineer to join our team on an independent contract basis, 40 hours per week for 6-12 months. This is a mid to senior level position and requires 4-5+ years of relevant experience. This role is flexible and 100% remote, but candidates MUST reside in the United States to be considered. Who are we? We are The Motley Fool, a purpose-driven financial information and services firm with more than 30 years of experience focused on making the world smarter, happier, and richer. But what does that even mean?! It means we're helping Fools (always with a capital \"F\") demystify the world of finance, beat the stock market, and achieve personal wealth and happiness through our products and services. The Motley Fool is firmly committed to diversity, inclusion, and equity. We are a motley group of overachievers that have built a culture of trust founded on Foolishness, fun, and a commitment to making the world smarter, happier and richer. However you identify or whatever winding road has led you to us, please don't hesitate to apply if the description above leaves you thinking, \"Hey! I could do that!\" What does this team do? The Data Engineering team at The Motley Fool creates data pipelines to wrangle data from around the Fool. We collaborate with everyone - from third party vendors to stakeholders to build easily consumable data structures for reporting and business insights. While working closely with our business analysts and machine learning specialists, we serve the data needs of all The Motley Fool Teams! What would you do in this role? As a Freelance Data Engineer, you will be responsible for expanding and optimizing data, the data pipeline architecture, the data flow, and collection for cross-functional teams. You are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. You will help to guide and support our software developers, database architects, data analysts, and data scientists on business initiatives while ensuring optimal data delivery architecture is consistent. Whether it's working on a solo project or with the team, you are self-directed and comfortable supporting the data needs of multiple teams, systems, and products. But What Would You Actually Do in this role? Leverage data assets to meet mission needs, ensuring consistent data quality, establishing data standards and governance Work in an agile, collaborative environment, partnering with client stakeholders, to develop and improve mission-based solutions Monitor cloud-based systems and components for availability, performance, reliability, security and efficiency Create and configure appropriate cloud resources to meet the needs of the end users. Strong, proven problem-solving skills and a proven ability to apply critical/analytical thinking to deliver sustainable and creative solutions to complex requirements. As needed, document topology, processes, and solution architecture. Assist with the training and enablement of data consumers. Share your passion for staying on top of tech trends, experimenting with and learning new technologies Required Experience: 3+ years data modeling experience 3+ years with Python 2+ years of experience with AWS Services Experience with AWS Glue and AWS Athena Experience with cloud data storage and compute components including lambda functions, EC2s, containers. Ability to work independently, and deliver results and drive projects with minimal supervision Strong ability to communicate blockers and issues to management for escalation and timely resolution Strong team player, with desire to learn new skills and broaden experience Experience working with complex data sets Preferred Qualifications: 2+ years with Snowflake 2+ years with Apache Spark 1+ years with Apache Airflow Experience working with Financial data Experience investing and/or using The Motley Fool's service offerings Hourly Pay Range $85—$105 USD By applying on this site, you acknowledge that The Motley Fool will be collecting the personal data you provide for our recruiting purposes. Please see our Applicant Privacy Notice for additional information about how we process, transfer, and store your data, including where that data is stored, and about any additional privacy rights you may have based on your jurisdiction.",$85 - $105 an hour,2024-01-14
TRA'BIAN ENTERPRISES,"Location: Dallas, TX Position Type: Contract Position Term: 1 Year All submitted candidates must be on our W2. Candidates can must be able to relocate after Covid. SKILLS: Data Engineer – 10 years experience Enterprise Content Management Documentum Captiva tool Javascript",DATA ENGINEER,"Dallas, TX","Location: Dallas, TX Position Type: Contract Position Term: 1 Year All submitted candidates must be on our W2. Candidates can must be able to relocate after Covid. SKILLS: Data Engineer – 10 years experience Enterprise Content Management Documentum Captiva tool Javascript",Competitive,2024-01-14
BayOne,"Job Description – The Marketing Analytic Systems data pipeline delivers value to various business partners by providing tools and capabilities to analyze large data sets for research, analytics, campaign management, reporting and tactical/strategic decision making. Essential Functions: Design, develop and implement end-to[1]end solutions on google cloud platform - big query, cloud composer, apache airflow; strong ability to translate business requirements into technical design plan. Automate, deploy and support solutions scheduled on Crontab or Control-M. Deployment includes proper error handling, dependency controls and necessary alerts. Triage and resolve production issues and identify preventive controls. Build rapid prototypes or proof of concepts for project feasibility. Document technical design specifications explaining how business and functional requirements are met. Document operations run book procedures with each solution deployment. Identify and propose improvements for analytics eco-system solution design and architecture. Participate in product support such patches and release upgrades. Provide validation support for Google cloud and SAS products, including any changes to other infrastructure, systems, processes that impact Analytics infrastructure. Participate in full SDLC framework using Agile/Lean methodology. Support non-production environments with the Operations and IT teams. Consistently demonstrate regular, dependable attendance & punctuality. Perform other duties as assigned. Top MUST have skills: Our systems development team is looking for a Developer who has passion to work with data and to build solutions that supports our Analytic Systems Solution stack, which includes Google cloud platform (GCP), SAS (Linux environment) and Tableau Server/Desktop. The successful candidate would have extensive experience as a data engineer or ETL developer building and automating data transformation and loading procedures. Strong knowledge and experience using Big Query, SQL, cloud composer, Apache airflow and SAS to conduct data profiling/discovery, data modelling and process automation is required. The candidate must be comfortable working with data from multiple sources; Hadoop, DB2, Oracle, flat files. The projects are detail intensive, requiring the accurate capture and translation of data requirements (both tactical and analytical needs) and validation of the working solution. We work in a highly collaborative environment working closely with cross functional team members; Business Analysts, Product Managers, Data Analysts and Report Developers Desired/nice to have skills: Education Requirements Minimum 4 Year / Bachelors Degree in Computer Science/Engineering, Analytics, Statistics or equivalent work experience. Years Of Experience Minimum Years of Experience – 5 Years Benefits Offered: Types of Insurance: Medical, Dental, Vision, Health Savings Account, Limited FSA, Life Insurance, Short Term Disability, Long Term Disability, Accidental Death, Critical Illness, Hospital Indemnity, Travel Accident, Pet Insurance, Home & Auto Coverage, Identity Theft Protection, Dependent Day Care Types of Paid Time Off: Paid Holidays, Vacation, Jury Duty, Holiday Worked Premium, Bereavement, Parental Leave Other Benefits: 401K, Bonuses based on Performance, Colleague Discount, Paid Adoption Assistance, Tuition Assistance, Matching Gift Program, Leave Sharing, Commuter/Transit, Employee Assistance Program, Group Legal Plan, 529 College Savings Plan, Direct Deposit, Retail Discounts ***Eligibility requirements apply to some benefits and may depend on location, job classification and length of employment. Benefits are subject to change and may be subject to specific plan or program terms.***",Data Engineer (Offshore),"Duluth, GA","Job Description – The Marketing Analytic Systems data pipeline delivers value to various business partners by providing tools and capabilities to analyze large data sets for research, analytics, campaign management, reporting and tactical/strategic decision making. Essential Functions: Design, develop and implement end-to[1]end solutions on google cloud platform - big query, cloud composer, apache airflow; strong ability to translate business requirements into technical design plan. Automate, deploy and support solutions scheduled on Crontab or Control-M. Deployment includes proper error handling, dependency controls and necessary alerts. Triage and resolve production issues and identify preventive controls. Build rapid prototypes or proof of concepts for project feasibility. Document technical design specifications explaining how business and functional requirements are met. Document operations run book procedures with each solution deployment. Identify and propose improvements for analytics eco-system solution design and architecture. Participate in product support such patches and release upgrades. Provide validation support for Google cloud and SAS products, including any changes to other infrastructure, systems, processes that impact Analytics infrastructure. Participate in full SDLC framework using Agile/Lean methodology. Support non-production environments with the Operations and IT teams. Consistently demonstrate regular, dependable attendance & punctuality. Perform other duties as assigned. Top MUST have skills: Our systems development team is looking for a Developer who has passion to work with data and to build solutions that supports our Analytic Systems Solution stack, which includes Google cloud platform (GCP), SAS (Linux environment) and Tableau Server/Desktop. The successful candidate would have extensive experience as a data engineer or ETL developer building and automating data transformation and loading procedures. Strong knowledge and experience using Big Query, SQL, cloud composer, Apache airflow and SAS to conduct data profiling/discovery, data modelling and process automation is required. The candidate must be comfortable working with data from multiple sources; Hadoop, DB2, Oracle, flat files. The projects are detail intensive, requiring the accurate capture and translation of data requirements (both tactical and analytical needs) and validation of the working solution. We work in a highly collaborative environment working closely with cross functional team members; Business Analysts, Product Managers, Data Analysts and Report Developers Desired/nice to have skills: Education Requirements Minimum 4 Year / Bachelors Degree in Computer Science/Engineering, Analytics, Statistics or equivalent work experience. Years Of Experience Minimum Years of Experience – 5 Years Benefits Offered: Types of Insurance: Medical, Dental, Vision, Health Savings Account, Limited FSA, Life Insurance, Short Term Disability, Long Term Disability, Accidental Death, Critical Illness, Hospital Indemnity, Travel Accident, Pet Insurance, Home & Auto Coverage, Identity Theft Protection, Dependent Day Care Types of Paid Time Off: Paid Holidays, Vacation, Jury Duty, Holiday Worked Premium, Bereavement, Parental Leave Other Benefits: 401K, Bonuses based on Performance, Colleague Discount, Paid Adoption Assistance, Tuition Assistance, Matching Gift Program, Leave Sharing, Commuter/Transit, Employee Assistance Program, Group Legal Plan, 529 College Savings Plan, Direct Deposit, Retail Discounts ***Eligibility requirements apply to some benefits and may depend on location, job classification and length of employment. Benefits are subject to change and may be subject to specific plan or program terms.***",Competitive,2024-01-14
Sephora,"Job ID: 240167 Location Name: FSC REMOTE SF/NY/DC -173(USA_0173) Address: FSC, Remote, CA 94105, United States (US) Job Type: Full Time Position Type: Regular Job Function: Information Technology Remote Eligible:Yes Company Overview: At Sephora we inspire our customers, empower our teams, and help them become the best versions of themselves. We create an environment where people are valued, and differences are celebrated. Every day, our teams across the world bring to life our purpose: to expand the way the world sees beauty by empowering the Extra Ordinary in each of us. We are united by a common goal - to reimagine the future of beauty. The Opportunity: Technology Our technology team works fast and smart. With San Francisco as our home, we take bringing new tech to market seriously, developing the latest in mobile technologies, scalable architecture, and the coolest in-store client experience. We love what we do and we have fun doing it. The Technology group is comprised of motivated self-starters and true team players that are absolutely integral to the growth of Sephora and our future success. Your role at Sephora: As a Sr. Software Engineer you will design and implement innovative analytical solutions and work alongside the product engineering team, evaluating new features and architecture. Reporting to the Engineering Manager, Data Platform, you will work closely with other team members like data architects and business analysts to understand what the business is trying to achieve, move data from source to target, and design optimal data models. You will be also responsible for building and maintaining the data platform. This hands-on technical role demands excellent knowledge and can demonstrate best practices in the industry. Come be a part of a team that is starting this new journey. Responsibilities: Streamline the intake of the raw data into our Azure Data lake. Perform production support and deployment activities. Proactively drive the execution of our core data engineering, business intelligence, and data warehouse frameworks Build data pipelines from systems such as CRM, Ecommerce etc. with the emphasis on scalability and reliability Leverage central data warehouse along with other data sources to create enriched customer information into CRM system Understand and translate business needs into data models to support long-term, scalable, and reliable solutions Create logical and physical data models using best practices to ensure high data quality and reduced redundancy Drive data quality across the organization; develop best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking Define and manage SLA’s for data sets and processes running in production Continuously improve our data infrastructure and stay ahead of technology Design a system for data backup in case of system failure Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and deliver on those needs We are excited about you if you have: 5+ years of experience with large scale data warehouse projects . Advanced Proficiency in Scala, Spark and SQL. Excellent knowledge in data structures and design patterns Preferred experience with data integration tools Experience working with Retail/CRM/Finance datasets preferred Knowledge in designing facts, dimensions, snapshots, SCDs, etc Write SQL for processing raw data, data validation and QA Knowledge working with APIs to collect or ingest data Strong Database knowledge, SQL & No-SQL preferred Communication Skills Data Engineers are part of a team, working with database administrators, data analysts and management and need to be effective communicators. Attention to Detail Databases are complex, and a minute error can cause huge problems. Problem-Solving Skills Data Engineers look at an issue that needs to be solved and come up with solutions quickly. Experience designing and implementing cloud based and SaaS data warehouse (e.g. WS, Hadoop, NoSQL) and developing ETL/ELT pipelines Experience integrating and building data platform in support of BI, Analytics, Data Science, and real-time applications Strong communication skills, with the ability to initiate and drive projects proactively and accurately The annual base salary range for this position is $132,624.00 - $174,990.00 The actual base salary offered depends on a variety of factors, which may include, as applicable, the applicant’s qualifications for the position; years of relevant experience; specific and unique skills; level of education attained; certifications or other professional licenses held; other legitimate, non-discriminatory business factors specific to the position; and the geographic location in which the applicant lives and/or from which they will perform the job. Individuals employed in this position may also be eligible to earn bonuses. Sephora offers a generous benefits package to full-time employees, which includes comprehensive health, dental and vision plans; a superior 401(k) plan, various paid time off programs; employee discount/perks; life insurance; disability insurance; flexible spending accounts; and an employee referral bonus program. While at Sephora, you’ll enjoy… The people. You will be surrounded by some of the most talented leaders and teams – people you can be proud to work with. The learning. We invest in training and developing our teams, and you will continue evolving and building your skills through personalized career plans. The culture. As a leading beauty retailer within the LVMH family, our reach is broad, and our impact is global. It is in our DNA to innovate and, at Sephora, all 40,000 passionate team members across 35 markets and 3,000+ stores, are united by a common goal - to reimagine the future of beauty. You can unleash your creativity, because we’ve got disruptive spirit. You can learn and evolve, because we empower you to be your best. You can be yourself, because you are what sets us apart. This, is the future of beauty. Reimagine your future, at Sephora. Sephora is an equal opportunity employer and values diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, ancestry, citizenship, gender, gender identity, sexual orientation, age, marital status, military/veteran status, or disability status. Sephora is committed to working with and providing reasonable accommodation to applicants with physical and mental disabilities. Sephora will consider for employment all qualified applicants with criminal histories in a manner consistent with applicable law.","Senior Engineer, Data Engineering",Remote in California,"Job ID: 240167 Location Name: FSC REMOTE SF/NY/DC -173(USA_0173) Address: FSC, Remote, CA 94105, United States (US) Job Type: Full Time Position Type: Regular Job Function: Information Technology Remote Eligible:Yes Company Overview: At Sephora we inspire our customers, empower our teams, and help them become the best versions of themselves. We create an environment where people are valued, and differences are celebrated. Every day, our teams across the world bring to life our purpose: to expand the way the world sees beauty by empowering the Extra Ordinary in each of us. We are united by a common goal - to reimagine the future of beauty. The Opportunity: Technology Our technology team works fast and smart. With San Francisco as our home, we take bringing new tech to market seriously, developing the latest in mobile technologies, scalable architecture, and the coolest in-store client experience. We love what we do and we have fun doing it. The Technology group is comprised of motivated self-starters and true team players that are absolutely integral to the growth of Sephora and our future success. Your role at Sephora: As a Sr. Software Engineer you will design and implement innovative analytical solutions and work alongside the product engineering team, evaluating new features and architecture. Reporting to the Engineering Manager, Data Platform, you will work closely with other team members like data architects and business analysts to understand what the business is trying to achieve, move data from source to target, and design optimal data models. You will be also responsible for building and maintaining the data platform. This hands-on technical role demands excellent knowledge and can demonstrate best practices in the industry. Come be a part of a team that is starting this new journey. Responsibilities: Streamline the intake of the raw data into our Azure Data lake. Perform production support and deployment activities. Proactively drive the execution of our core data engineering, business intelligence, and data warehouse frameworks Build data pipelines from systems such as CRM, Ecommerce etc. with the emphasis on scalability and reliability Leverage central data warehouse along with other data sources to create enriched customer information into CRM system Understand and translate business needs into data models to support long-term, scalable, and reliable solutions Create logical and physical data models using best practices to ensure high data quality and reduced redundancy Drive data quality across the organization; develop best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking Define and manage SLA’s for data sets and processes running in production Continuously improve our data infrastructure and stay ahead of technology Design a system for data backup in case of system failure Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and deliver on those needs We are excited about you if you have: 5+ years of experience with large scale data warehouse projects . Advanced Proficiency in Scala, Spark and SQL. Excellent knowledge in data structures and design patterns Preferred experience with data integration tools Experience working with Retail/CRM/Finance datasets preferred Knowledge in designing facts, dimensions, snapshots, SCDs, etc Write SQL for processing raw data, data validation and QA Knowledge working with APIs to collect or ingest data Strong Database knowledge, SQL & No-SQL preferred Communication Skills Data Engineers are part of a team, working with database administrators, data analysts and management and need to be effective communicators. Attention to Detail Databases are complex, and a minute error can cause huge problems. Problem-Solving Skills Data Engineers look at an issue that needs to be solved and come up with solutions quickly. Experience designing and implementing cloud based and SaaS data warehouse (e.g. WS, Hadoop, NoSQL) and developing ETL/ELT pipelines Experience integrating and building data platform in support of BI, Analytics, Data Science, and real-time applications Strong communication skills, with the ability to initiate and drive projects proactively and accurately The annual base salary range for this position is $132,624.00 - $174,990.00 The actual base salary offered depends on a variety of factors, which may include, as applicable, the applicant’s qualifications for the position; years of relevant experience; specific and unique skills; level of education attained; certifications or other professional licenses held; other legitimate, non-discriminatory business factors specific to the position; and the geographic location in which the applicant lives and/or from which they will perform the job. Individuals employed in this position may also be eligible to earn bonuses. Sephora offers a generous benefits package to full-time employees, which includes comprehensive health, dental and vision plans; a superior 401(k) plan, various paid time off programs; employee discount/perks; life insurance; disability insurance; flexible spending accounts; and an employee referral bonus program. While at Sephora, you’ll enjoy… The people. You will be surrounded by some of the most talented leaders and teams – people you can be proud to work with. The learning. We invest in training and developing our teams, and you will continue evolving and building your skills through personalized career plans. The culture. As a leading beauty retailer within the LVMH family, our reach is broad, and our impact is global. It is in our DNA to innovate and, at Sephora, all 40,000 passionate team members across 35 markets and 3,000+ stores, are united by a common goal - to reimagine the future of beauty. You can unleash your creativity, because we’ve got disruptive spirit. You can learn and evolve, because we empower you to be your best. You can be yourself, because you are what sets us apart. This, is the future of beauty. Reimagine your future, at Sephora. Sephora is an equal opportunity employer and values diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, ancestry, citizenship, gender, gender identity, sexual orientation, age, marital status, military/veteran status, or disability status. Sephora is committed to working with and providing reasonable accommodation to applicants with physical and mental disabilities. Sephora will consider for employment all qualified applicants with criminal histories in a manner consistent with applicable law.",Competitive,2024-01-14
BayOne,"We have a urgent requirement for Data Platform Engineer with Gurbhud. This associate must have good experience with Kafka Client : Grubhub Location : NY City, NY(Remote till COVID is done) Rate : $85 to $90 an hour. Grubhub is looking for an innately curious, business-minded, results-oriented Senior Software Engineer with a focus on data intensive applications, specifically Kafka and other streaming applications. In this role you will build and deploy platform-level tools and applications that enable a variety of data products across the Grubhub enterprise. You will be working alongside other platform engineers to help the company improve its data products and analytics. Specific responsibilities include, but are not limited to, development of streaming applications, problem solving, performance analysis, data model tooling (protobuf/avro/json-schema) and data pipeline tooling using cutting edge technologies on AWS. Why Work For Us We have a fast-paced environment and that is what our teams thrive on. Grubhub believes in empowering people and offering opportunities for development, as well as professional growth. We value strong, positive relationships in all areas: with each other, our customers and our greater community. Want to be a part of a team of diverse collaborators in an authentically fun culture? If so, we want to talk to you - and hear what's your favorite restaurant for food delivery! The Impact You Will Make Understand the business! Work directly with application developers, data engineers and data analysts to address their needs. Build streaming data applications. Be a stakeholder for our data platform tools! Help guide and prioritize development of the tooling to enhance our capabilities. Help build a multi-datacenter, performant and highly available data platform and the frameworks to support it. Help build and support frameworks to interact with various cloud technologies. Actively contribute to the adoption of strong software architecture, the development of best practices, and new technologies. We are always improving the process of building software; we need you to help contribute Help train software developers and other technologists on using the data platform, software stack (e.g. Kafka, Presto, Spark) to build their product-specific applications. Relentlessly analyze and improve the performance of our systems. You Should Have Bachelor's Degree in Science, Programming or Engineering related field. 5+ years experience building highly-scalable, highly-available applications. 1-2 years of experience with Streaming technologies (Kafka, Beam, Flink, Spark). expertise in Java, Scala, Python or a similar modern object-oriented language Data querying capabilities using SQL Ability to explain technical concepts in simple terms to business stakeholders Knowledge of or experience with developing distributed systems Experience with End-to-End (E2) testing frameworks Experience mentoring/coaching engineers along with overseeing the technical work of developers from other teams. A knack for analyzing and improving processes using data Got These? Even Better Experience designing and implementing multi-region streaming and data applications. Experience deploying data pipelines in a production environment Experience with distributed data and computing tools like Spark, Hive and Presto Experience using cloud infrastructure like AWS",Data Platform Engineer,"Santa Clara, CA","We have a urgent requirement for Data Platform Engineer with Gurbhud. This associate must have good experience with Kafka Client : Grubhub Location : NY City, NY(Remote till COVID is done) Rate : $85 to $90 an hour. Grubhub is looking for an innately curious, business-minded, results-oriented Senior Software Engineer with a focus on data intensive applications, specifically Kafka and other streaming applications. In this role you will build and deploy platform-level tools and applications that enable a variety of data products across the Grubhub enterprise. You will be working alongside other platform engineers to help the company improve its data products and analytics. Specific responsibilities include, but are not limited to, development of streaming applications, problem solving, performance analysis, data model tooling (protobuf/avro/json-schema) and data pipeline tooling using cutting edge technologies on AWS. Why Work For Us We have a fast-paced environment and that is what our teams thrive on. Grubhub believes in empowering people and offering opportunities for development, as well as professional growth. We value strong, positive relationships in all areas: with each other, our customers and our greater community. Want to be a part of a team of diverse collaborators in an authentically fun culture? If so, we want to talk to you - and hear what's your favorite restaurant for food delivery! The Impact You Will Make Understand the business! Work directly with application developers, data engineers and data analysts to address their needs. Build streaming data applications. Be a stakeholder for our data platform tools! Help guide and prioritize development of the tooling to enhance our capabilities. Help build a multi-datacenter, performant and highly available data platform and the frameworks to support it. Help build and support frameworks to interact with various cloud technologies. Actively contribute to the adoption of strong software architecture, the development of best practices, and new technologies. We are always improving the process of building software; we need you to help contribute Help train software developers and other technologists on using the data platform, software stack (e.g. Kafka, Presto, Spark) to build their product-specific applications. Relentlessly analyze and improve the performance of our systems. You Should Have Bachelor's Degree in Science, Programming or Engineering related field. 5+ years experience building highly-scalable, highly-available applications. 1-2 years of experience with Streaming technologies (Kafka, Beam, Flink, Spark). expertise in Java, Scala, Python or a similar modern object-oriented language Data querying capabilities using SQL Ability to explain technical concepts in simple terms to business stakeholders Knowledge of or experience with developing distributed systems Experience with End-to-End (E2) testing frameworks Experience mentoring/coaching engineers along with overseeing the technical work of developers from other teams. A knack for analyzing and improving processes using data Got These? Even Better Experience designing and implementing multi-region streaming and data applications. Experience deploying data pipelines in a production environment Experience with distributed data and computing tools like Spark, Hive and Presto Experience using cloud infrastructure like AWS",$85 - $90 an hour,2024-01-14
Match Group,"Hinge is the dating app designed to be deleted In today's digital world, finding genuine relationships is tougher than ever. At Hinge, we’re on a mission to inspire intimate connection to create a less lonely world. We’re obsessed with understanding our users’ behaviors to help them find love, and our success is defined by one simple metric– setting up great dates. With tens of millions of users across the globe, we’ve become the most trusted way to find a relationship, for all. About the Role: As a Data Engineer at Hinge, you will create essential data processes and contribute to components of a modern data pipeline that will be the foundation of Hinge’s decision-making ability. The systems you help create, the problems you help solve, and the support of our analytical minds, will be pivotal to the success of Hinge. You will be making a real impact on the data platform team and will be key to the success of Hinge. Your work will enable the organization to make data-driven decisions and drive improvements, which will affect the love lives of tons of people. Data Engineers will be improving core functionality and implementing critical pipelines that will not only aid the data engineering team, but the rest of the organization. As a Data Engineer at Hinge, you will be working on an actual big data architecture, while concentrating on real-world problems. Responsibilities: Work with our data teams to ensure data is flowing accurately through data creation to our presentation layers. Improve our Data Engineering stack through containerization, data modeling, developing our ETL pipelines, and more. Use your expertise in data modeling to design, build and maintain our analytics data warehouse that provides clean, accurate, and robust data sets to be leveraged for reporting, analytics, and machine learning initiatives. Work with stakeholders and translate their needs and expectations into action items and deliverables. Continue to learn more about the Data Engineering discipline, utilize that knowledge in your deliverables, and identify opportunities to enhance our pipelines. Contribute meaningful insights and feedback to our team processes. Participate in our on-call rotation. What We're Looking For: Proficient in Python, SQL, DevOps, and databases. 3+ years of professional/industry experience. Experience delivering data products from conception to delivery and with the infrastructure that supports their underlying processes. Strong communication skills (written/verbal).Experience modeling data sets for different types of sources and business processes. Passionate about designing elegant data infrastructure, tooling, and pipelines. Familiar with our stack: Kubernetes, Docker, Terraform, Kafka, Airflow, dbt, Looker, AWS technologies (S3, Redshift), GCP technologies (Dataflow, BigQuery), and CI/CD technologies (CircleCI). Factors such as scope and responsibilities of the position, candidate's work experience, education/training, job-related skills, internal peer equity, as well as market and business considerations may influence base pay offered. This salary range is reflective of a position based in New York City. This salary will be subject to a geographic adjustment (according to a specific city and state), if an authorization is granted to work outside of the location listed in this posting. As a member of our team, you’ll enjoy: 401(k) Matching: We match 100% of the first 10% of pre-tax 401(k) contributions you make, up to a maximum of $10,000 per year. Professional Growth: Get a $3,000 annual Learning & Development stipend once you’ve been with us for three months. You also get free access to Udemy, an online learning and teaching marketplace with over 6000 courses, starting your first day. Parental Leave & Planning: When you become a new parent, you’re eligible for 100% paid parental leave (20 paid weeks for both birth and non-birth parents.) Fertility Support: You’ll get easy access to fertility care through Carrot, from basic treatments to fertility preservation. We also provide $10,000 toward fertility preservation. You and your spouse/domestic partner are both eligible. Date Stipend: All Hinge employees receive a $100 monthly stipend for epic dates– Romantic or otherwise. Hinge Premium is also free for employees and their loved ones. ERGs: We have eight Employee Resource Groups (ERGs)—Asian, Unapologetic, Disability, LGBTQIA+, Vibras, Women/Nonbinary, Parents, and Remote—that hold regular meetings, host events, and provide dedicated support to the organization & its community. At Hinge, our core values are… Authenticity: We share, never hide, our words, actions and intentions. Courage: We embrace lofty goals and tough challenges. Empathy: We deeply consider the perspective of others. Diversity inspires innovation Hinge is an equal-opportunity employer. We value diversity at our company and do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We believe success is created by a diverse workforce of individuals with different ideas, strengths, interests, and cultural backgrounds.",Data Engineer III,"Hybrid remote in New York, NY","Hinge is the dating app designed to be deleted In today's digital world, finding genuine relationships is tougher than ever. At Hinge, we’re on a mission to inspire intimate connection to create a less lonely world. We’re obsessed with understanding our users’ behaviors to help them find love, and our success is defined by one simple metric– setting up great dates. With tens of millions of users across the globe, we’ve become the most trusted way to find a relationship, for all. About the Role: As a Data Engineer at Hinge, you will create essential data processes and contribute to components of a modern data pipeline that will be the foundation of Hinge’s decision-making ability. The systems you help create, the problems you help solve, and the support of our analytical minds, will be pivotal to the success of Hinge. You will be making a real impact on the data platform team and will be key to the success of Hinge. Your work will enable the organization to make data-driven decisions and drive improvements, which will affect the love lives of tons of people. Data Engineers will be improving core functionality and implementing critical pipelines that will not only aid the data engineering team, but the rest of the organization. As a Data Engineer at Hinge, you will be working on an actual big data architecture, while concentrating on real-world problems. Responsibilities: Work with our data teams to ensure data is flowing accurately through data creation to our presentation layers. Improve our Data Engineering stack through containerization, data modeling, developing our ETL pipelines, and more. Use your expertise in data modeling to design, build and maintain our analytics data warehouse that provides clean, accurate, and robust data sets to be leveraged for reporting, analytics, and machine learning initiatives. Work with stakeholders and translate their needs and expectations into action items and deliverables. Continue to learn more about the Data Engineering discipline, utilize that knowledge in your deliverables, and identify opportunities to enhance our pipelines. Contribute meaningful insights and feedback to our team processes. Participate in our on-call rotation. What We're Looking For: Proficient in Python, SQL, DevOps, and databases. 3+ years of professional/industry experience. Experience delivering data products from conception to delivery and with the infrastructure that supports their underlying processes. Strong communication skills (written/verbal).Experience modeling data sets for different types of sources and business processes. Passionate about designing elegant data infrastructure, tooling, and pipelines. Familiar with our stack: Kubernetes, Docker, Terraform, Kafka, Airflow, dbt, Looker, AWS technologies (S3, Redshift), GCP technologies (Dataflow, BigQuery), and CI/CD technologies (CircleCI). Factors such as scope and responsibilities of the position, candidate's work experience, education/training, job-related skills, internal peer equity, as well as market and business considerations may influence base pay offered. This salary range is reflective of a position based in New York City. This salary will be subject to a geographic adjustment (according to a specific city and state), if an authorization is granted to work outside of the location listed in this posting. As a member of our team, you’ll enjoy: 401(k) Matching: We match 100% of the first 10% of pre-tax 401(k) contributions you make, up to a maximum of $10,000 per year. Professional Growth: Get a $3,000 annual Learning & Development stipend once you’ve been with us for three months. You also get free access to Udemy, an online learning and teaching marketplace with over 6000 courses, starting your first day. Parental Leave & Planning: When you become a new parent, you’re eligible for 100% paid parental leave (20 paid weeks for both birth and non-birth parents.) Fertility Support: You’ll get easy access to fertility care through Carrot, from basic treatments to fertility preservation. We also provide $10,000 toward fertility preservation. You and your spouse/domestic partner are both eligible. Date Stipend: All Hinge employees receive a $100 monthly stipend for epic dates– Romantic or otherwise. Hinge Premium is also free for employees and their loved ones. ERGs: We have eight Employee Resource Groups (ERGs)—Asian, Unapologetic, Disability, LGBTQIA+, Vibras, Women/Nonbinary, Parents, and Remote—that hold regular meetings, host events, and provide dedicated support to the organization & its community. At Hinge, our core values are… Authenticity: We share, never hide, our words, actions and intentions. Courage: We embrace lofty goals and tough challenges. Empathy: We deeply consider the perspective of others. Diversity inspires innovation Hinge is an equal-opportunity employer. We value diversity at our company and do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We believe success is created by a diverse workforce of individuals with different ideas, strengths, interests, and cultural backgrounds.","$143,000 - $172,000 a year",2024-01-14
Netflix,"Remote, United States Data Science and Engineering At Netflix, our mission is to entertain the world. With 200+ million paid members in over 190 countries on millions of devices; enjoying TV series, documentaries, and feature films across a wide variety of genres and languages - Netflix is reinventing entertainment from end to end. We are revolutionizing how shows and movies are produced, pushing technological boundaries to efficiently deliver streaming video at a massive scale over the internet, and continuously improving the end-to-end user experience with Netflix across their member journey. We pride ourselves on using data to inform our decision-making as we work towards our mission. This requires curating data across various domains such as Growth, Finance, Product, Content, and Studio. All of this data collection and curation is made possible thanks to the amazing Data Engineers of Netflix who bring this data to life. Data Engineering at Netflix is a role that requires building systems to process data efficiently and modeling the data to power analytics. These solutions can range from batch data pipelines that bring to life business metrics to real-time processing services that integrate with our core product features. In addition, we require our Data Engineers to have a rich understanding of large distributed systems on which our data solutions rely. Candidates should have knowledge across several of these skill sets and usually need to be deep in at least one. As a Data Engineer, you also need to have strong communication skills since you will need to collaborate with business, engineering, and data science teams to enable a culture of learning. Learn more about the work of data engineers at Netflix. Location of work: We are considering candidates who are willing to relocate to Los Gatos, California, as well as fully-remote candidates (remote in the US with occasional visits to Los Gatos) depending on the team your skills are most aligned with. Who are you? You strive to write elegant code, and you're comfortable with picking up new technologies independently You are proficient in at least one major programming language (e.g. Java, Scala, Python) and comfortable working with SQL You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine learning models You have a strong background in at least one of the following: distributed data processing or software engineering of data services, or data modeling You are familiar with big data technologies like Spark or Flink and comfortable working with web-scale datasets You have an eye for detail, good data intuition, and a passion for data quality You appreciate the importance of great documentation and data debugging skills You relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and giving/receiving candid feedback You are comfortable working in a rapidly changing environment with ambiguous requirements. You are nimble and take intelligent risks Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $150,000 - $750,000. Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here. Netflix is a unique culture and environment. Learn more here.",Data Engineer (L5),Remote,"Remote, United States Data Science and Engineering At Netflix, our mission is to entertain the world. With 200+ million paid members in over 190 countries on millions of devices; enjoying TV series, documentaries, and feature films across a wide variety of genres and languages - Netflix is reinventing entertainment from end to end. We are revolutionizing how shows and movies are produced, pushing technological boundaries to efficiently deliver streaming video at a massive scale over the internet, and continuously improving the end-to-end user experience with Netflix across their member journey. We pride ourselves on using data to inform our decision-making as we work towards our mission. This requires curating data across various domains such as Growth, Finance, Product, Content, and Studio. All of this data collection and curation is made possible thanks to the amazing Data Engineers of Netflix who bring this data to life. Data Engineering at Netflix is a role that requires building systems to process data efficiently and modeling the data to power analytics. These solutions can range from batch data pipelines that bring to life business metrics to real-time processing services that integrate with our core product features. In addition, we require our Data Engineers to have a rich understanding of large distributed systems on which our data solutions rely. Candidates should have knowledge across several of these skill sets and usually need to be deep in at least one. As a Data Engineer, you also need to have strong communication skills since you will need to collaborate with business, engineering, and data science teams to enable a culture of learning. Learn more about the work of data engineers at Netflix. Location of work: We are considering candidates who are willing to relocate to Los Gatos, California, as well as fully-remote candidates (remote in the US with occasional visits to Los Gatos) depending on the team your skills are most aligned with. Who are you? You strive to write elegant code, and you're comfortable with picking up new technologies independently You are proficient in at least one major programming language (e.g. Java, Scala, Python) and comfortable working with SQL You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine learning models You have a strong background in at least one of the following: distributed data processing or software engineering of data services, or data modeling You are familiar with big data technologies like Spark or Flink and comfortable working with web-scale datasets You have an eye for detail, good data intuition, and a passion for data quality You appreciate the importance of great documentation and data debugging skills You relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and giving/receiving candid feedback You are comfortable working in a rapidly changing environment with ambiguous requirements. You are nimble and take intelligent risks Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $150,000 - $750,000. Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here. Netflix is a unique culture and environment. Learn more here.",Competitive,2024-01-14
Providence,"Description This position is remote and can sit in the footprint of Providence in the states of AK, WA, OR, CA, TX and MT. The Data Engineer designs and builds extract code, data pipelines and transformations, data enrichment processes, provisioning layers, and secure transmission methods to support clinical and operational processes across all parts of the healthcare system. This person will place a priority on collaboration with meticulous source control and documentation, an emphasis on simple solutions to complex problems, the ability to extract very specific data in defined formats for secure transmission. Knowledge of healthcare data and experience with data extraction from an EMR essential. Providence caregivers are not simply valued – they’re invaluable. Join our team at Enterprise Information Services and thrive in our culture of patient-focused, whole-person care built on understanding, commitment, and mutual respect. Your voice matters here, because we know that to inspire and retain the best people, we must empower them. Required Qualifications: Bachelor's Degree in Computer Engineering, Computer Science, Mathematics, Engineering or equivalent education/experience 1 year related experience; 1-3 years preferred Healthcare data knowledge and experience Preferred Qualifications: EPIC certification(s) The salary range listed for this position MIN:$43.39 to MAX:$70.03 per hour is based upon the primary work location Beaverton, OR posted. This position is remote. Salary range and offers are determined by internal pay equity and geographic cost of living differences. Salary range will vary from State and region. Salary max is limited to 75% range in order to continue to offer internal pay growth. We welcome open and transparent discussions on salary at Providence. Why Join Providence? Our best-in-class benefits are uniquely designed to support you and your family in staying well, growing professionally, and achieving financial security. We take care of you, so you can focus on delivering our Mission of caring for everyone, especially the most vulnerable in our communities. Accepting a new position at another facility that is part of the Providence family of organizations may change your current benefits. Changes in benefits, including paid time-off, happen for various reasons. These reasons can include changes of Legal Employer, FTE, Union, location, time-off plan policies, availability of health and welfare benefit plan offerings, and other various reasons. About Providence At Providence, our strength lies in Our Promise of “Know me, care for me, ease my way.” Working at our family of organizations means that regardless of your role, we’ll walk alongside you in your career, supporting you so you can support others. We provide best-in-class benefits and we foster an inclusive workplace where diversity is valued, and everyone is essential, heard and respected. Together, our 120,000 caregivers (all employees) serve in over 50 hospitals, over 1,000 clinics and a full range of health and social services across Alaska, California, Montana, New Mexico, Oregon, Texas and Washington. As a comprehensive health care organization, we are serving more people, advancing best practices and continuing our more than 100-year tradition of serving the poor and vulnerable. The amounts listed are the base pay range; additional compensation may be available for this role, such as shift differentials, standby/on-call, overtime, premiums, extra shift incentives, or bonus opportunities. Check out our benefits page for more information about our Benefits and Rewards. About the Team Providence Shared Services is a service line within Providence that provides a variety of functional and system support services for our family of organizations across Alaska, California, Montana, New Mexico, Oregon, Texas and Washington. We are focused on supporting our Mission by delivering a robust foundation of services and sharing of specialized expertise. We are committed to the principle that every workforce member has the right to work in surroundings that are free from all forms of unlawful discrimination and harassment. We are committed to cultural diversity and equal employment for all individuals. It is our policy to recruit, hire, promote, compensate, transfer, train, retain, terminate, and make all other employment-related decisions without regard to race, color, religious creed (including religious dress and grooming practices), national origin (including certain language use restrictions), ancestry, disability (mental and physical including HIV and AIDS), medical condition (including cancer and genetic characteristics), genetic information, marital status, age, sex (which includes pregnancy, childbirth, breastfeeding and related medical conditions), gender, gender identity, gender expression, sexual orientation, genetic information, and military and veteran status or any other applicable legally protected status. We will also provide reasonable accommodation to known physical or mental limitations of an otherwise qualified caregiver or applicant for employment, unless the accommodation would impose undue hardship on the operation of our business. We are a community where all people, regardless of differences, are welcome, secure, and valued. We value respect, appreciation, collaboration, diversity, and a shared commitment to serving our communities. We expect that all workforce members in our community will act in ways which reflect a commitment to and accountability for, racial and social justice and equality in the workplace. As such, we will maintain a workplace free of discrimination and harassment based on any applicable legally protected status. We also expect that all workforce members will maintain a positive workplace free from any unacceptable conduct which creates an intimidating, hostile, or offensive work environment. Requsition ID: 248645 Company: Providence Jobs Job Category: Development/Engineering Job Function: Information Technology Job Schedule: Full time Job Shift: Career Track: Department: 4011 SS IS HI DP 3 Address: OR Beaverton 3601 SW Murray Blvd Work Location: Murray Business Ctr Beaverton-Beaverton Pay Range: $43.39 - $70.03 The amounts listed are the base pay range; additional compensation may be available for this role, such as shift differentials, standby/on-call, overtime, premiums, extra shift incentives, or bonus opportunities. Check out our benefits page for more information about our Benefits and Rewards.",Health Care Data Engineer I IS,Remote in Washington State,"Description This position is remote and can sit in the footprint of Providence in the states of AK, WA, OR, CA, TX and MT. The Data Engineer designs and builds extract code, data pipelines and transformations, data enrichment processes, provisioning layers, and secure transmission methods to support clinical and operational processes across all parts of the healthcare system. This person will place a priority on collaboration with meticulous source control and documentation, an emphasis on simple solutions to complex problems, the ability to extract very specific data in defined formats for secure transmission. Knowledge of healthcare data and experience with data extraction from an EMR essential. Providence caregivers are not simply valued – they’re invaluable. Join our team at Enterprise Information Services and thrive in our culture of patient-focused, whole-person care built on understanding, commitment, and mutual respect. Your voice matters here, because we know that to inspire and retain the best people, we must empower them. Required Qualifications: Bachelor's Degree in Computer Engineering, Computer Science, Mathematics, Engineering or equivalent education/experience 1 year related experience; 1-3 years preferred Healthcare data knowledge and experience Preferred Qualifications: EPIC certification(s) The salary range listed for this position MIN:$43.39 to MAX:$70.03 per hour is based upon the primary work location Beaverton, OR posted. This position is remote. Salary range and offers are determined by internal pay equity and geographic cost of living differences. Salary range will vary from State and region. Salary max is limited to 75% range in order to continue to offer internal pay growth. We welcome open and transparent discussions on salary at Providence. Why Join Providence? Our best-in-class benefits are uniquely designed to support you and your family in staying well, growing professionally, and achieving financial security. We take care of you, so you can focus on delivering our Mission of caring for everyone, especially the most vulnerable in our communities. Accepting a new position at another facility that is part of the Providence family of organizations may change your current benefits. Changes in benefits, including paid time-off, happen for various reasons. These reasons can include changes of Legal Employer, FTE, Union, location, time-off plan policies, availability of health and welfare benefit plan offerings, and other various reasons. About Providence At Providence, our strength lies in Our Promise of “Know me, care for me, ease my way.” Working at our family of organizations means that regardless of your role, we’ll walk alongside you in your career, supporting you so you can support others. We provide best-in-class benefits and we foster an inclusive workplace where diversity is valued, and everyone is essential, heard and respected. Together, our 120,000 caregivers (all employees) serve in over 50 hospitals, over 1,000 clinics and a full range of health and social services across Alaska, California, Montana, New Mexico, Oregon, Texas and Washington. As a comprehensive health care organization, we are serving more people, advancing best practices and continuing our more than 100-year tradition of serving the poor and vulnerable. The amounts listed are the base pay range; additional compensation may be available for this role, such as shift differentials, standby/on-call, overtime, premiums, extra shift incentives, or bonus opportunities. Check out our benefits page for more information about our Benefits and Rewards. About the Team Providence Shared Services is a service line within Providence that provides a variety of functional and system support services for our family of organizations across Alaska, California, Montana, New Mexico, Oregon, Texas and Washington. We are focused on supporting our Mission by delivering a robust foundation of services and sharing of specialized expertise. We are committed to the principle that every workforce member has the right to work in surroundings that are free from all forms of unlawful discrimination and harassment. We are committed to cultural diversity and equal employment for all individuals. It is our policy to recruit, hire, promote, compensate, transfer, train, retain, terminate, and make all other employment-related decisions without regard to race, color, religious creed (including religious dress and grooming practices), national origin (including certain language use restrictions), ancestry, disability (mental and physical including HIV and AIDS), medical condition (including cancer and genetic characteristics), genetic information, marital status, age, sex (which includes pregnancy, childbirth, breastfeeding and related medical conditions), gender, gender identity, gender expression, sexual orientation, genetic information, and military and veteran status or any other applicable legally protected status. We will also provide reasonable accommodation to known physical or mental limitations of an otherwise qualified caregiver or applicant for employment, unless the accommodation would impose undue hardship on the operation of our business. We are a community where all people, regardless of differences, are welcome, secure, and valued. We value respect, appreciation, collaboration, diversity, and a shared commitment to serving our communities. We expect that all workforce members in our community will act in ways which reflect a commitment to and accountability for, racial and social justice and equality in the workplace. As such, we will maintain a workplace free of discrimination and harassment based on any applicable legally protected status. We also expect that all workforce members will maintain a positive workplace free from any unacceptable conduct which creates an intimidating, hostile, or offensive work environment. Requsition ID: 248645 Company: Providence Jobs Job Category: Development/Engineering Job Function: Information Technology Job Schedule: Full time Job Shift: Career Track: Department: 4011 SS IS HI DP 3 Address: OR Beaverton 3601 SW Murray Blvd Work Location: Murray Business Ctr Beaverton-Beaverton Pay Range: $43.39 - $70.03 The amounts listed are the base pay range; additional compensation may be available for this role, such as shift differentials, standby/on-call, overtime, premiums, extra shift incentives, or bonus opportunities. Check out our benefits page for more information about our Benefits and Rewards.",Competitive,2024-01-14
The Cincinnati Insurance Companies,"US-Remote Description Make a difference with a career in insurance At The Cincinnati Insurance Companies, we put people first and apply the Golden Rule to our daily operations. To put this into action, we’re looking for extraordinary people to join our talented team. Our service-oriented, ethical, knowledgeable, caring associates are the heart of our vision to be the best company serving independent agents. We help protect families and businesses as they work to prevent or recover from a loss. Share your talents to help us reach for continued success as we bring value to our communities and demonstrate that Actions Speak Louder in Person®. If you’re ready to build productive relationships, collaborate within a diverse team, embrace challenges and develop your skills, then Cincinnati may be the place for you. We offer career opportunities where you can contribute and continue to grow. Build your future with us Our IT Business Intelligence and Data Management department is currently seeking an experienced data engineer to build data pipelines that provide peace of mind for people by equipping them to make data-informed business decisions. Starting Pay: The pay range for this position is $65,000 - $132,000 annually. The pay determination is based on the applicant’s education, experience, location, knowledge, skills, and abilities. Eligible associates may also receive an annual cash bonus and stock incentives based on company and individual performance. Be ready to: develop reliable, reusable, automated and streamlined ETL code analyze and organize raw data from multiple sources to produce requested or required data elements design table structures and ETL to build performant data solutions that are scalable in a fast growing data ecosystem create reconciliation procedures and data checks to ensure data quality build in automated audit, balancing and controls provide production support, including on-call rotation and technical assistance to end-user and IT staff lead user requirements gathering and documentation designs proactively conduct data investigations and assist business partners with complex data analysis and ad-hoc queries lead application and technical architecture analysis, design and implementation, and ensure BI product fulfills requirements lead efforts with data management and database specialists to troubleshoot and report issues design new reporting applications to ensure reporting requests can be supported by the underlying data source adhere to all metadata management and data quality standards by leading quality management reviews understand configuration dependencies and interrelationships between data warehouse and business intelligence tools and designs; troubleshoot in the area with this knowledge proactively identify, build and maintain relationships with business process owners and colleagues identify changes in scope or work effort that could result in budgetary overrun or missing of delivery date, design and implement contingency demonstrate expert problem solving capabilities Be equipped with: proven experience in designing and implementing ETL processes (preferred tools: IBM's DataStage, SQL, SAS, Python) proven experience distilling business requirements into data warehouse design artifacts such as a Facts Qualifier Matrix (FQM), source to target maps and star/snowflake schema data models prefer five or more years experience with SQL, Data Modeling and Dimensional Modeling understanding of enterprise technical landscape (property and casualty insurance background a plus) knowledge of developing BI dashboards (preferred tools: IBM's Cognos, Microsoft's Power BI); creating data quality scorecards and data lineage experience with agile teams mentor development team members in design and development of BI solutions strong team working skills, an analytical nature, self-motivated and excellent written and oral communication skills ability to comprehend complex technical and logical concepts and adapt quickly to changes ability to maintain own workflow and meet deadlines while managing parallel project deliverables with minimal direction ability to create effective visual models to enable collective understanding of processes, data flow, user interaction and others as needed prior reporting application support experience track record of successfully architecting small, medium and enterprise-scale solutions preferred three or more years of business requirements experience Bring your experience or education from: a bachelor's degree or have equivalent experience in computer science or related discipline. Enhance your talents Providing outstanding service and developing strong relationships with our independent agents are hallmarks of our company. Whether you have experience from another carrier or you’re new to the insurance industry, we promote a lifelong learning approach. Cincinnati provides you with the tools and training to be successful and to become a trusted, respected insurance professional – all while enjoying a meaningful career. Enjoy benefits and amenities Your commitment to providing strong service, sharing best practices, and creating solutions that impact lives is appreciated. To increase the well-being and satisfaction of our associates, we offer a variety of benefits and amenities. Embrace a diverse team As a relationship-based organization, we welcome and value a diverse workforce. We provide equal employment opportunity to all qualified persons without regard to race; creed; color; sex, including sexual orientation, gender identity and transgender status; religion; national origin; age; disability; military service; veteran status; pregnancy; AIDS/HIV or genetic information; or any other basis prohibited by law. .",IT - Data Engineer II - IV (Remote),Remote,"US-Remote Description Make a difference with a career in insurance At The Cincinnati Insurance Companies, we put people first and apply the Golden Rule to our daily operations. To put this into action, we’re looking for extraordinary people to join our talented team. Our service-oriented, ethical, knowledgeable, caring associates are the heart of our vision to be the best company serving independent agents. We help protect families and businesses as they work to prevent or recover from a loss. Share your talents to help us reach for continued success as we bring value to our communities and demonstrate that Actions Speak Louder in Person®. If you’re ready to build productive relationships, collaborate within a diverse team, embrace challenges and develop your skills, then Cincinnati may be the place for you. We offer career opportunities where you can contribute and continue to grow. Build your future with us Our IT Business Intelligence and Data Management department is currently seeking an experienced data engineer to build data pipelines that provide peace of mind for people by equipping them to make data-informed business decisions. Starting Pay: The pay range for this position is $65,000 - $132,000 annually. The pay determination is based on the applicant’s education, experience, location, knowledge, skills, and abilities. Eligible associates may also receive an annual cash bonus and stock incentives based on company and individual performance. Be ready to: develop reliable, reusable, automated and streamlined ETL code analyze and organize raw data from multiple sources to produce requested or required data elements design table structures and ETL to build performant data solutions that are scalable in a fast growing data ecosystem create reconciliation procedures and data checks to ensure data quality build in automated audit, balancing and controls provide production support, including on-call rotation and technical assistance to end-user and IT staff lead user requirements gathering and documentation designs proactively conduct data investigations and assist business partners with complex data analysis and ad-hoc queries lead application and technical architecture analysis, design and implementation, and ensure BI product fulfills requirements lead efforts with data management and database specialists to troubleshoot and report issues design new reporting applications to ensure reporting requests can be supported by the underlying data source adhere to all metadata management and data quality standards by leading quality management reviews understand configuration dependencies and interrelationships between data warehouse and business intelligence tools and designs; troubleshoot in the area with this knowledge proactively identify, build and maintain relationships with business process owners and colleagues identify changes in scope or work effort that could result in budgetary overrun or missing of delivery date, design and implement contingency demonstrate expert problem solving capabilities Be equipped with: proven experience in designing and implementing ETL processes (preferred tools: IBM's DataStage, SQL, SAS, Python) proven experience distilling business requirements into data warehouse design artifacts such as a Facts Qualifier Matrix (FQM), source to target maps and star/snowflake schema data models prefer five or more years experience with SQL, Data Modeling and Dimensional Modeling understanding of enterprise technical landscape (property and casualty insurance background a plus) knowledge of developing BI dashboards (preferred tools: IBM's Cognos, Microsoft's Power BI); creating data quality scorecards and data lineage experience with agile teams mentor development team members in design and development of BI solutions strong team working skills, an analytical nature, self-motivated and excellent written and oral communication skills ability to comprehend complex technical and logical concepts and adapt quickly to changes ability to maintain own workflow and meet deadlines while managing parallel project deliverables with minimal direction ability to create effective visual models to enable collective understanding of processes, data flow, user interaction and others as needed prior reporting application support experience track record of successfully architecting small, medium and enterprise-scale solutions preferred three or more years of business requirements experience Bring your experience or education from: a bachelor's degree or have equivalent experience in computer science or related discipline. Enhance your talents Providing outstanding service and developing strong relationships with our independent agents are hallmarks of our company. Whether you have experience from another carrier or you’re new to the insurance industry, we promote a lifelong learning approach. Cincinnati provides you with the tools and training to be successful and to become a trusted, respected insurance professional – all while enjoying a meaningful career. Enjoy benefits and amenities Your commitment to providing strong service, sharing best practices, and creating solutions that impact lives is appreciated. To increase the well-being and satisfaction of our associates, we offer a variety of benefits and amenities. Embrace a diverse team As a relationship-based organization, we welcome and value a diverse workforce. We provide equal employment opportunity to all qualified persons without regard to race; creed; color; sex, including sexual orientation, gender identity and transgender status; religion; national origin; age; disability; military service; veteran status; pregnancy; AIDS/HIV or genetic information; or any other basis prohibited by law. .","$65,000 - $132,000 a year",2024-01-14
Mass General Brigham,"Sr. Data Engineer - (3273355) About Us: As a not-for-profit organization, Mass General Brigham is committed to supporting patient care, research, teaching, and service to the community by leading innovation across our system. Founded by Brigham and Women’s Hospital and Massachusetts General Hospital, Mass General Brigham supports a complete continuum of care including community and specialty hospitals, a managed care organization, a physician network, community health centers, home care, and other health-related entities. Several of our hospitals are teaching affiliates of Harvard Medical School, and our system is a national leader in biomedical research. We’re focused on a people-first culture for our system’s patients and our professional family. That’s why we provide our employees with more ways to achieve their potential. Mass General Brigham is committed to aligning our employees’ personal aspirations with projects that match their capabilities and creating a culture that empowers our managers to become trusted mentors. We support each member of our team to own their personal development—and we recognize success at every step. Our employees use the Mass General Brigham values to govern decisions, actions, and behaviors. These values guide how we get our work done: Patients, Affordability, Accountability & Service Commitment, Decisiveness, Innovation & Thoughtful Risk; and how we treat each other: Diversity & Inclusion, Integrity & Respect, Learning, Continuous Improvement & Personal Growth, Teamwork & Collaboration. Principal Duties and Responsibilities: Perform EDW ETL/ELT ingestions and integrations to ensure support data and analytic needs Build and enhance standard solution that provides efficient and scalable ETL/ELT solutions from multiple data sources Ensure the quality of data assets and robustness of data engineering processes Experience with change control, release management and other ITIL processes Participate in building out EDW on Snowflake, expanding and optimizing the data ecosystem, as well as optimizing data engineering processes Support BI Developers, Data Architects, Data Analysts and Data Scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects Conduct ETL solution, code, and pre-prod review to ensure optimal data delivery architecture is consistent throughout ongoing projects Collaborate with Platform Architect on ETL/ELT new features and train peers Perform troubleshooting on ETLs and related components Identify application bottlenecks and opportunities to optimize performance Informatica PowerCenter base system configuration and support including installation, upgrades, performance tuning, etc. Working Conditions: The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. This position requires occasional local travel to MGB sites, vendors, and/or conferences Hospital work environment working conditions include possible exposure to diseases or infections and may require safety gear (PPE) such as gloves and mask. Normal office working conditions. The noise level in the work environment is quiet to moderate. While performing the duties of this job, the employee is frequently required to sit; talk; or hear; use hands to finger; handle; or feel; reach with hands and arms. The employee is occasionally required to stand; walk; and stoop; kneel; or crouch. The employee must frequently lift and/or move up to 5 pounds and occasionally lift and/or move up to 20 pounds. Specific vision abilities required by this job include close vision, distance vision and depth perception. Qualifications 7+ plus years of experience designing and building data ingestions and data integration solution for Enterprise Data and Analytics Solutions 2+ years of experience with developing models in DBT Experience with Azure DevOps, Azure Git and CI/CD data pipeline integrations. Demonstrates good knowledge of cloud computing platforms such as AWS, GCP, and Azure Experience of support and enhance existing Enterprise Data Warehouse or Data Lake Ensure designed systems are highly reliable, self-recovering, and require little or no supporting manpower Familiar with change control, release management, and other ITIL methodology Deep experience in traditional Data Warehousing & Cloud data warehousing Mastery of SQL, especially within cloud-based data warehouses like Snowflake Professional knowledge of Azure Experience developing data pipelines using Snowflake features (Snow pipe, SnowSQL, Data Streams, Snow Sight) Ability to clearly and concisely communicate complex technical concepts to both technical and non-technical audiences Proven verbal, communication, and presentation skills Proven ability to work independently Skills/Abilities/Competencies: Thorough knowledge of Snowflake, DBT, Azure Microsoft SQL Server, SSIS, and related ETL tools Strong SQL skills and workload management on Snowflake Ability to troubleshoot system bottleneck and conduct performance tuning in large data warehouse or data lake Experience with logical and physical data modeling Experience with real time data processing and analytics products is a plus 5+ plus years of experience with building larger data warehouse in SQL Server, SnowSQL is a must 2+ years of experience with developing models in DBT Familiarity with data engineering, data governance and data quality principles Healthcare experience, most notably in Clinical data, Epic, Payer data and reference data is a plus but not mandatory EEO Statement Mass General Brigham is an Equal Opportunity Employer. By embracing diverse skills, perspectives, and ideas, we choose to lead. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, national origin, sex, age, gender identity, disability, sexual orientation, military service, genetic information, and/or other status protected under the law. We will ensure that all individuals with a disability are provided a reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment. Primary Location MA-Somerville-MGB Assembly Row Work Locations MGB Assembly Row 399 Revolution Drive Somerville 02145 Job Business and Systems Analyst Organization Mass General Brigham Schedule Full-time Standard Hours 40 Shift Day Job Posted Shift Description remote Eastern business hours Full Time Employee Status Regular Recruiting Department MGB Digital Job Posting Jan 11, 2024",Sr. Data Engineer,"Somerville, MA 02145 (Assembly Square area)","Sr. Data Engineer - (3273355) About Us: As a not-for-profit organization, Mass General Brigham is committed to supporting patient care, research, teaching, and service to the community by leading innovation across our system. Founded by Brigham and Women’s Hospital and Massachusetts General Hospital, Mass General Brigham supports a complete continuum of care including community and specialty hospitals, a managed care organization, a physician network, community health centers, home care, and other health-related entities. Several of our hospitals are teaching affiliates of Harvard Medical School, and our system is a national leader in biomedical research. We’re focused on a people-first culture for our system’s patients and our professional family. That’s why we provide our employees with more ways to achieve their potential. Mass General Brigham is committed to aligning our employees’ personal aspirations with projects that match their capabilities and creating a culture that empowers our managers to become trusted mentors. We support each member of our team to own their personal development—and we recognize success at every step. Our employees use the Mass General Brigham values to govern decisions, actions, and behaviors. These values guide how we get our work done: Patients, Affordability, Accountability & Service Commitment, Decisiveness, Innovation & Thoughtful Risk; and how we treat each other: Diversity & Inclusion, Integrity & Respect, Learning, Continuous Improvement & Personal Growth, Teamwork & Collaboration. Principal Duties and Responsibilities: Perform EDW ETL/ELT ingestions and integrations to ensure support data and analytic needs Build and enhance standard solution that provides efficient and scalable ETL/ELT solutions from multiple data sources Ensure the quality of data assets and robustness of data engineering processes Experience with change control, release management and other ITIL processes Participate in building out EDW on Snowflake, expanding and optimizing the data ecosystem, as well as optimizing data engineering processes Support BI Developers, Data Architects, Data Analysts and Data Scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects Conduct ETL solution, code, and pre-prod review to ensure optimal data delivery architecture is consistent throughout ongoing projects Collaborate with Platform Architect on ETL/ELT new features and train peers Perform troubleshooting on ETLs and related components Identify application bottlenecks and opportunities to optimize performance Informatica PowerCenter base system configuration and support including installation, upgrades, performance tuning, etc. Working Conditions: The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. This position requires occasional local travel to MGB sites, vendors, and/or conferences Hospital work environment working conditions include possible exposure to diseases or infections and may require safety gear (PPE) such as gloves and mask. Normal office working conditions. The noise level in the work environment is quiet to moderate. While performing the duties of this job, the employee is frequently required to sit; talk; or hear; use hands to finger; handle; or feel; reach with hands and arms. The employee is occasionally required to stand; walk; and stoop; kneel; or crouch. The employee must frequently lift and/or move up to 5 pounds and occasionally lift and/or move up to 20 pounds. Specific vision abilities required by this job include close vision, distance vision and depth perception. Qualifications 7+ plus years of experience designing and building data ingestions and data integration solution for Enterprise Data and Analytics Solutions 2+ years of experience with developing models in DBT Experience with Azure DevOps, Azure Git and CI/CD data pipeline integrations. Demonstrates good knowledge of cloud computing platforms such as AWS, GCP, and Azure Experience of support and enhance existing Enterprise Data Warehouse or Data Lake Ensure designed systems are highly reliable, self-recovering, and require little or no supporting manpower Familiar with change control, release management, and other ITIL methodology Deep experience in traditional Data Warehousing & Cloud data warehousing Mastery of SQL, especially within cloud-based data warehouses like Snowflake Professional knowledge of Azure Experience developing data pipelines using Snowflake features (Snow pipe, SnowSQL, Data Streams, Snow Sight) Ability to clearly and concisely communicate complex technical concepts to both technical and non-technical audiences Proven verbal, communication, and presentation skills Proven ability to work independently Skills/Abilities/Competencies: Thorough knowledge of Snowflake, DBT, Azure Microsoft SQL Server, SSIS, and related ETL tools Strong SQL skills and workload management on Snowflake Ability to troubleshoot system bottleneck and conduct performance tuning in large data warehouse or data lake Experience with logical and physical data modeling Experience with real time data processing and analytics products is a plus 5+ plus years of experience with building larger data warehouse in SQL Server, SnowSQL is a must 2+ years of experience with developing models in DBT Familiarity with data engineering, data governance and data quality principles Healthcare experience, most notably in Clinical data, Epic, Payer data and reference data is a plus but not mandatory EEO Statement Mass General Brigham is an Equal Opportunity Employer. By embracing diverse skills, perspectives, and ideas, we choose to lead. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, national origin, sex, age, gender identity, disability, sexual orientation, military service, genetic information, and/or other status protected under the law. We will ensure that all individuals with a disability are provided a reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment. Primary Location MA-Somerville-MGB Assembly Row Work Locations MGB Assembly Row 399 Revolution Drive Somerville 02145 Job Business and Systems Analyst Organization Mass General Brigham Schedule Full-time Standard Hours 40 Shift Day Job Posted Shift Description remote Eastern business hours Full Time Employee Status Regular Recruiting Department MGB Digital Job Posting Jan 11, 2024",Competitive,2024-01-14
Frost Bank,"It’s about putting our best to the test. Are you described as someone with an inquisitive mind and an innovative personality? Are you never satisfied with good enough? Does solving complex problems and ensuring top-quality systems excite you? If so, being a Data Engineer III with Frost could be for you. At Frost, it’s about more than a job. It’s about having a flourishing career where you can thrive, both in and out of work. At Frost, we’re committed to fostering an environment that reflects our values and encourages team members to be the best they can be. In joining our adaptable, integrity-driven team, you’ll become part of Frost’s over 150-year legacy of providing unparalleled banking services. Who you are: As a Data Engineer III, you will lead the development and implementation of ETL processes and data pipelines. You’ll play an important role in designing and building scalable and reliable data infrastructures. You’ll use your strong problem-solving skills to ensure that the systems are performing optimally and meet our high standards. You believe in effective communication and will have the opportunity to address potential problems and solutions to complex issues. What you’ll do: Develop and maintain data pipelines to automate data ingestion and processing Collaborate with stakeholders to identify data requirements and ensure data infrastructure meets business needs Develop and enforce data governance policies and standards Monitor data infrastructure and performance to identify and resolve issues Drive best practices via code and design reviews Coach, mentor, and provide technical assessments Provide guidance to other Data Engineers as needed Stay up to date with industry trends and new technologies in data engineering Always take action using integrity, caring, and excellence to achieve all-win outcomes What you’ll need: Bachelor's degree in Computer Science, Information Technology, or related field 4+ Years of experience as a data engineer 3+ Years of experience developing in either Python, Java , Scala or Spark Advanced understanding of database technologies such as SQL and NoSQL Expertise in ETL tools and techniques Knowledge of data modeling and schema design Strong problem-solving and analytical skills Familiarity with Big Data technologies such as Hadoop, Spark, Hive, and Cloud native data engineering technologies Experience leveraging cloud technologies to develop data pipelines Mastery of data modeling concepts Strong understanding of ETL concepts and Data Warehousing concepts Experience with CI/CD pipelines Experience with version control software Strong understanding of Agile Principles (Scrum) Excellent written and verbal communication skills Additional Preferred Skills: Know how of Informatica Intelligent Cloud Services (IICS) will be a plus. Our Benefits: At Frost, we care about your health, your family, and your future and strive to have our benefits reflect that. This includes: Medical, dental, vision, long-term disability, and life insurance 401(k) matching Generous holiday and paid time off schedule Tuition reimbursement Extensive health and wellness programs, including our Employee Assistance Program Referral bonus program + more! Since 1868, Frost has dedicated their expertise to provide exceptional banking, investment, and insurance services to businesses and individuals throughout Texas. Frost is one of the 50 largest U.S. banks by asset size and is a leader in banking customer satisfaction. At Frost, it’s about being part of something bigger. If this sounds like you, we encourage you to apply and see what’s possible at Frost. #LI-MF1 #LI-Hybrid",DATA ENGINEER III,"San Antonio, TX","It’s about putting our best to the test. Are you described as someone with an inquisitive mind and an innovative personality? Are you never satisfied with good enough? Does solving complex problems and ensuring top-quality systems excite you? If so, being a Data Engineer III with Frost could be for you. At Frost, it’s about more than a job. It’s about having a flourishing career where you can thrive, both in and out of work. At Frost, we’re committed to fostering an environment that reflects our values and encourages team members to be the best they can be. In joining our adaptable, integrity-driven team, you’ll become part of Frost’s over 150-year legacy of providing unparalleled banking services. Who you are: As a Data Engineer III, you will lead the development and implementation of ETL processes and data pipelines. You’ll play an important role in designing and building scalable and reliable data infrastructures. You’ll use your strong problem-solving skills to ensure that the systems are performing optimally and meet our high standards. You believe in effective communication and will have the opportunity to address potential problems and solutions to complex issues. What you’ll do: Develop and maintain data pipelines to automate data ingestion and processing Collaborate with stakeholders to identify data requirements and ensure data infrastructure meets business needs Develop and enforce data governance policies and standards Monitor data infrastructure and performance to identify and resolve issues Drive best practices via code and design reviews Coach, mentor, and provide technical assessments Provide guidance to other Data Engineers as needed Stay up to date with industry trends and new technologies in data engineering Always take action using integrity, caring, and excellence to achieve all-win outcomes What you’ll need: Bachelor's degree in Computer Science, Information Technology, or related field 4+ Years of experience as a data engineer 3+ Years of experience developing in either Python, Java , Scala or Spark Advanced understanding of database technologies such as SQL and NoSQL Expertise in ETL tools and techniques Knowledge of data modeling and schema design Strong problem-solving and analytical skills Familiarity with Big Data technologies such as Hadoop, Spark, Hive, and Cloud native data engineering technologies Experience leveraging cloud technologies to develop data pipelines Mastery of data modeling concepts Strong understanding of ETL concepts and Data Warehousing concepts Experience with CI/CD pipelines Experience with version control software Strong understanding of Agile Principles (Scrum) Excellent written and verbal communication skills Additional Preferred Skills: Know how of Informatica Intelligent Cloud Services (IICS) will be a plus. Our Benefits: At Frost, we care about your health, your family, and your future and strive to have our benefits reflect that. This includes: Medical, dental, vision, long-term disability, and life insurance 401(k) matching Generous holiday and paid time off schedule Tuition reimbursement Extensive health and wellness programs, including our Employee Assistance Program Referral bonus program + more! Since 1868, Frost has dedicated their expertise to provide exceptional banking, investment, and insurance services to businesses and individuals throughout Texas. Frost is one of the 50 largest U.S. banks by asset size and is a leader in banking customer satisfaction. At Frost, it’s about being part of something bigger. If this sounds like you, we encourage you to apply and see what’s possible at Frost. #LI-MF1 #LI-Hybrid",Competitive,2024-01-14
BayOne,"Data Warehousing Engineers Engage with business leaders to gather requirements and translate to scalable data warehousing solutions: Design storage, schema & data models using Cloud data warehouse technologies (F1, Plx. BigQuery) Build security models, data infrastructure, system integration in compliance with Cloud access policies Create data sets and business metrics Build data pipelines & automated workflows Design Looker LookML models and complex dashboards Deliver data solutions using Agile methodology Design storage, schema & data models using Cloud data warehouse technologies (F1, Plx. Big Query)",Data Warehousing Engineer,"Mountain View, CA","Data Warehousing Engineers Engage with business leaders to gather requirements and translate to scalable data warehousing solutions: Design storage, schema & data models using Cloud data warehouse technologies (F1, Plx. BigQuery) Build security models, data infrastructure, system integration in compliance with Cloud access policies Create data sets and business metrics Build data pipelines & automated workflows Design Looker LookML models and complex dashboards Deliver data solutions using Agile methodology Design storage, schema & data models using Cloud data warehouse technologies (F1, Plx. Big Query)",Competitive,2024-01-14
Dassault Systèmes,"Medidata: Powering Smarter Treatments and Healthier People Medidata, a Dassault Systèmes company, is leading the digital transformation of life sciences, creating hope for millions of people. Medidata helps generate the evidence and insights to help pharmaceutical, biotech, medical device and diagnostics companies, and academic researchers accelerate value, minimize risk, and optimize outcomes. More than one million registered users across 2,000+ customers and partners access the world's most trusted platform for clinical development, commercial, and real-world data. Known for its ground-breaking technological innovations, Medidata has supported more than 30,000 clinical trials and 9 million study participants. And Medidata's ongoing commitment to infusing the patient voice into trial designs and solutions is creating a better and more inclusive experience for all participants in clinical studies. Medidata is involved in nearly 40% of company-initiated trial starts globally, with studies conducted in more than 140 countries. More than 70% of novel drugs approved by the Food and Drug Administration (FDA) in 2022 were developed with Medidata software. Medidata is headquartered in New York City and has offices around the world to meet the needs of its customers. Discover more at www.medidata.com and follow us @medidata. Our Team: Medidata is looking for individuals who will help us solve some of the most complex questions facing the industry today using our proprietary platform and advanced analytics. Our users depend on our products to participate in any clinical trial using web, mobile and sensor-based solutions. We focus on the patient experience and aim to provide engaging solutions that fit into people's everyday lives. We make clinical trials faster by reducing burdens for both patients and study personnel. You will partner with all of the key stakeholder functions including product, delivery, data science, engineering, partnerships, and operational teams. What we're looking for: As a Senior Data Engineer, you are an important part of our team building strategic data pipelines into the Patient Cloud Data Platform. You will report to the Manager of Engineering and Testing and work with multiple teams daily. You are a motivated, technical data engineer with experience transforming data sets and will individually contribute in the following ways: Promote the data vision and roadmap to meet team objectives Identify the dependencies ahead and come up with proposed solutions. Implement data engineering strategies that align with our goals. Ensure data engineering processes are efficient, and compliant with regulatory requirements. Identify opportunities for process improvement and new technology adoption. Create new data pipelines and transform data to a structure that is relevant to the product need Analyze complex data structures, elements and systems to create performant pipelines Develop logical and physical data models that are efficient and best suited for the intended use Manage data reliability and engineering planning from requirements to deployment, including assessing end-user and our needs, promoting engagement, and keeping processes running smoothly. Enforces the implementation of best practices for data auditing, scalability, reliability, high availability and application performance. Apply data extraction, transformation and loading techniques to connect large datasets from a variety of sources. Be a mentor for junior and senior team members. Perform data analysis and testing to identify data gaps and ensure data quality. Work with application, data platform, and data engineering teams to reconfigure data ingestion pipelines to be more reliable and monitored Manage data incidents and lead blameless postmortems. Help build the creation of monitoring, alerting, and reporting on the reliability of data pipelines and other big data processing systems. Partner with product teams to develop a data management strategy following our goals Stay current with new technologies and data engineering concepts Follow Medidata's Standard Operating Procedures to ensure all software meets regulatory and company requirements Requirements (Education & Experience): Bachelor's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related field with at least 5+ years of data engineering experience in an enterprise environment Experience managing high-volume data processing streams Familiarity with building data visualizations and dashboards Experience building complex, automated data processes in Python, Spark, AWS, Airflow, Snowflake and other technologies Excellent knowledge of databases such as Postgres, Elasticsearch, Redshift, and Aurora, including distributed database design, SQL vs. NoSQL, and database optimizations. Experience with data engineering, database management, business intelligence and data transformation tools Experience developing web applications in frameworks like Shiny, Vue, React. Along with programming proficiency, must have a capacity for. Must have experience leading complex scientific data management and integration initiatives in a research environment. The salary range posted below refers only to positions that will be physically based in New York City. As with all roles, Medidata sets ranges based on a number of factors including function, level, candidate expertise and experience, and geographic location. Pay ranges for candidates in locations other than New York City, may differ based on the local market data in that region. The base salary pay range for this position is $114,750 to $153,000. Base pay is one part of the Total Rewards that Medidata provides to compensate and recognize employees for their work. Most sales positions are eligible for a commission on the terms of applicable plan documents, and many of Medidata's non-sales positions are eligible for annual bonuses. Medidata believes that benefits should connect you to the support you need when it matters most and provides best-in-class benefits, including medical, dental, life and disability insurance; 401(k) matching; unlimited paid time off; and 10 paid holidays per year. #LI-MM1 Diversity statement As a game-changer in sustainable technology and innovation, Dassault Systèmes is striving to build more inclusive and diverse teams across the globe. We believe that our people are our number one asset and we want all employees to feel empowered to bring their whole selves to work every day. It is our goal that our people feel a sense of pride and a passion for belonging. As a company leading change, it’s our responsibility to foster opportunities for all people to participate in a harmonized Workforce of the Future. Equal opportunity In order to provide equal employment and advancement opportunities to all individuals, employment decisions at 3DS are based on merit, qualifications and abilities. 3DS is committed to a policy of non-discrimination and equal opportunity for all employees and qualified applicants without regard to race, color, religion, gender, sex (including pregnancy, childbirth or medical or common conditions related to pregnancy or childbirth), sexual orientation, gender identity, gender expression, marital status, familial status, national origin, ancestry, age (40 and above), disability, veteran status, military service, application for military service, genetic information, receipt of free medical care, or any other characteristic protected under applicable law. 3DS will make reasonable accommodations for qualified individuals with known disabilities, in accordance with applicable law.",Senior Data Engineer,"New York, NY","Medidata: Powering Smarter Treatments and Healthier People Medidata, a Dassault Systèmes company, is leading the digital transformation of life sciences, creating hope for millions of people. Medidata helps generate the evidence and insights to help pharmaceutical, biotech, medical device and diagnostics companies, and academic researchers accelerate value, minimize risk, and optimize outcomes. More than one million registered users across 2,000+ customers and partners access the world's most trusted platform for clinical development, commercial, and real-world data. Known for its ground-breaking technological innovations, Medidata has supported more than 30,000 clinical trials and 9 million study participants. And Medidata's ongoing commitment to infusing the patient voice into trial designs and solutions is creating a better and more inclusive experience for all participants in clinical studies. Medidata is involved in nearly 40% of company-initiated trial starts globally, with studies conducted in more than 140 countries. More than 70% of novel drugs approved by the Food and Drug Administration (FDA) in 2022 were developed with Medidata software. Medidata is headquartered in New York City and has offices around the world to meet the needs of its customers. Discover more at www.medidata.com and follow us @medidata. Our Team: Medidata is looking for individuals who will help us solve some of the most complex questions facing the industry today using our proprietary platform and advanced analytics. Our users depend on our products to participate in any clinical trial using web, mobile and sensor-based solutions. We focus on the patient experience and aim to provide engaging solutions that fit into people's everyday lives. We make clinical trials faster by reducing burdens for both patients and study personnel. You will partner with all of the key stakeholder functions including product, delivery, data science, engineering, partnerships, and operational teams. What we're looking for: As a Senior Data Engineer, you are an important part of our team building strategic data pipelines into the Patient Cloud Data Platform. You will report to the Manager of Engineering and Testing and work with multiple teams daily. You are a motivated, technical data engineer with experience transforming data sets and will individually contribute in the following ways: Promote the data vision and roadmap to meet team objectives Identify the dependencies ahead and come up with proposed solutions. Implement data engineering strategies that align with our goals. Ensure data engineering processes are efficient, and compliant with regulatory requirements. Identify opportunities for process improvement and new technology adoption. Create new data pipelines and transform data to a structure that is relevant to the product need Analyze complex data structures, elements and systems to create performant pipelines Develop logical and physical data models that are efficient and best suited for the intended use Manage data reliability and engineering planning from requirements to deployment, including assessing end-user and our needs, promoting engagement, and keeping processes running smoothly. Enforces the implementation of best practices for data auditing, scalability, reliability, high availability and application performance. Apply data extraction, transformation and loading techniques to connect large datasets from a variety of sources. Be a mentor for junior and senior team members. Perform data analysis and testing to identify data gaps and ensure data quality. Work with application, data platform, and data engineering teams to reconfigure data ingestion pipelines to be more reliable and monitored Manage data incidents and lead blameless postmortems. Help build the creation of monitoring, alerting, and reporting on the reliability of data pipelines and other big data processing systems. Partner with product teams to develop a data management strategy following our goals Stay current with new technologies and data engineering concepts Follow Medidata's Standard Operating Procedures to ensure all software meets regulatory and company requirements Requirements (Education & Experience): Bachelor's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related field with at least 5+ years of data engineering experience in an enterprise environment Experience managing high-volume data processing streams Familiarity with building data visualizations and dashboards Experience building complex, automated data processes in Python, Spark, AWS, Airflow, Snowflake and other technologies Excellent knowledge of databases such as Postgres, Elasticsearch, Redshift, and Aurora, including distributed database design, SQL vs. NoSQL, and database optimizations. Experience with data engineering, database management, business intelligence and data transformation tools Experience developing web applications in frameworks like Shiny, Vue, React. Along with programming proficiency, must have a capacity for. Must have experience leading complex scientific data management and integration initiatives in a research environment. The salary range posted below refers only to positions that will be physically based in New York City. As with all roles, Medidata sets ranges based on a number of factors including function, level, candidate expertise and experience, and geographic location. Pay ranges for candidates in locations other than New York City, may differ based on the local market data in that region. The base salary pay range for this position is $114,750 to $153,000. Base pay is one part of the Total Rewards that Medidata provides to compensate and recognize employees for their work. Most sales positions are eligible for a commission on the terms of applicable plan documents, and many of Medidata's non-sales positions are eligible for annual bonuses. Medidata believes that benefits should connect you to the support you need when it matters most and provides best-in-class benefits, including medical, dental, life and disability insurance; 401(k) matching; unlimited paid time off; and 10 paid holidays per year. #LI-MM1 Diversity statement As a game-changer in sustainable technology and innovation, Dassault Systèmes is striving to build more inclusive and diverse teams across the globe. We believe that our people are our number one asset and we want all employees to feel empowered to bring their whole selves to work every day. It is our goal that our people feel a sense of pride and a passion for belonging. As a company leading change, it’s our responsibility to foster opportunities for all people to participate in a harmonized Workforce of the Future. Equal opportunity In order to provide equal employment and advancement opportunities to all individuals, employment decisions at 3DS are based on merit, qualifications and abilities. 3DS is committed to a policy of non-discrimination and equal opportunity for all employees and qualified applicants without regard to race, color, religion, gender, sex (including pregnancy, childbirth or medical or common conditions related to pregnancy or childbirth), sexual orientation, gender identity, gender expression, marital status, familial status, national origin, ancestry, age (40 and above), disability, veteran status, military service, application for military service, genetic information, receipt of free medical care, or any other characteristic protected under applicable law. 3DS will make reasonable accommodations for qualified individuals with known disabilities, in accordance with applicable law.","$114,750 - $153,000 a year",2024-01-14
Compass Corporate,"Compass Corporate Salary: $120000 - $140000 A family of companies and experiences As the leading foodservice and support services company, Compass Group USA is known for our great people, great service and our great results. If you’ve been hungry and away from home, chances are you’ve tasted Compass Group’s delicious food and experienced our outstanding service. Our 225,000 associates work in award-winning restaurants, corporate cafes, hospitals, schools, arenas, museums, and more in all 50 states. Our reach is constantly expanding to shape the industry and create new opportunities for innovation. Join the Compass family today! great people. great services. great results. Each and every individual plays a key role in the growth and legacy of our company. We know the next big idea can come from anyone. We encourage developing and attracting expertise that differentiates us as a company as we continue to raise the bar. Job Summary: We're looking for a hands-on, collaborative Data Engineer to join our Master Data Management team. In this role you'll have the opportunity to manage data from ingestion to data modeling. You will also develop the APIs to meet the needs of business intelligence and reporting solutions. We’re currently a Python and AWS focused team using a tech stack of Python, and a range of AWS services like S3, PostgreSQL, DynamoDB, Athena, Redshift, Snowflake, Lambda and Glue. Job Responsibilities: Define, build, test, and implement scalable data pipelines Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues Collaborate with development, analytics and reporting teams to develop data models that feed business intelligence tools Design and build API integrations to support the needs of analysts and reporting systems Develop, deploy, and manage AWS Lambda functions written in Python Develop, deploy, and manage AWS Glue jobs written in Python Ensuring efficient and scalable serverless operations Debugging and troubleshooting Lambda functions / Glue jobs Collaborating with other AWS service teams Job Qualifications: Bachelor’s degree in Computer Science, Information Systems, Analytics, or related field 3+ years of experience programming with Python 3+ years in an ETL or Data Engineering role building and implementing data pipelines Keen understanding of design best practices for multiple needs including OLTP systems, ODS reporting needs, and dimensional database practices Experience with AWS Lambda & AWS Glue Understanding of serverless architecture benefits and challenges Experience designing, building, and maintaining data processing pipelines Experience with API driven data access (API development experience a plus) Strong skills with Python and SQL with the ability to write efficient queries Experience working in agile environments Highly self-motivated and directed with an attention to detail Good understanding of relational theory, data warehousing design techniques, data modeling, and data design best practices Ability to communicate well with multiple groups to elicit and confirm data requirements Collaborating with other AWS Services Teams Apply to Compass Group today! Click here to Learn More about the Compass Story Compass Group is an equal opportunity employer. At Compass, we are committed to treating all Applicants and Associates fairly based on their abilities, achievements, and experience without regard to race, national origin, sex, age, disability, veteran status, sexual orientation, gender identity, or any other classification protected by law. Qualified candidates must be able to perform the essential functions of this position satisfactorily with or without a reasonable accommodation. Disclaimer: this job post is not necessarily an exhaustive list of all essential responsibilities, skills, tasks, or requirements associated with this position. While this is intended to be an accurate reflection of the position posted, the Company reserves the right to modify or change the essential functions of the job based on business necessity. We will consider for employment all qualified applicants, including those with a criminal history (including relevant driving history), in a manner consistent with all applicable federal, state, and local laws, including the City of Los Angeles’ Fair Chance Initiative for Hiring Ordinance, the San Francisco Fair Chance Ordinance, and the New York Fair Chance Act. We encourage applicants with a criminal history (and driving history) to apply. Compass Corporate maintains a drug-free workplace. Associates at Corporate are offered many fantastic benefits. Medical Dental Vision Life Insurance/ AD Disability Insurance Retirement Plan Paid Time Off Holiday Time Off (varies by site/state) Associate Shopping Program Health and Wellness Programs Discount Marketplace Identity Theft Protection Pet Insurance Commuter Benefits Employee Assistance Program Flexible Spending Accounts (FSAs) Req ID: 1251316 Compass Corporate Michelle Lombardozzi [[req_classification]]",PYTHON DATA ENGINEER (REMOTE),Remote,"Compass Corporate Salary: $120000 - $140000 A family of companies and experiences As the leading foodservice and support services company, Compass Group USA is known for our great people, great service and our great results. If you’ve been hungry and away from home, chances are you’ve tasted Compass Group’s delicious food and experienced our outstanding service. Our 225,000 associates work in award-winning restaurants, corporate cafes, hospitals, schools, arenas, museums, and more in all 50 states. Our reach is constantly expanding to shape the industry and create new opportunities for innovation. Join the Compass family today! great people. great services. great results. Each and every individual plays a key role in the growth and legacy of our company. We know the next big idea can come from anyone. We encourage developing and attracting expertise that differentiates us as a company as we continue to raise the bar. Job Summary: We're looking for a hands-on, collaborative Data Engineer to join our Master Data Management team. In this role you'll have the opportunity to manage data from ingestion to data modeling. You will also develop the APIs to meet the needs of business intelligence and reporting solutions. We’re currently a Python and AWS focused team using a tech stack of Python, and a range of AWS services like S3, PostgreSQL, DynamoDB, Athena, Redshift, Snowflake, Lambda and Glue. Job Responsibilities: Define, build, test, and implement scalable data pipelines Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues Collaborate with development, analytics and reporting teams to develop data models that feed business intelligence tools Design and build API integrations to support the needs of analysts and reporting systems Develop, deploy, and manage AWS Lambda functions written in Python Develop, deploy, and manage AWS Glue jobs written in Python Ensuring efficient and scalable serverless operations Debugging and troubleshooting Lambda functions / Glue jobs Collaborating with other AWS service teams Job Qualifications: Bachelor’s degree in Computer Science, Information Systems, Analytics, or related field 3+ years of experience programming with Python 3+ years in an ETL or Data Engineering role building and implementing data pipelines Keen understanding of design best practices for multiple needs including OLTP systems, ODS reporting needs, and dimensional database practices Experience with AWS Lambda & AWS Glue Understanding of serverless architecture benefits and challenges Experience designing, building, and maintaining data processing pipelines Experience with API driven data access (API development experience a plus) Strong skills with Python and SQL with the ability to write efficient queries Experience working in agile environments Highly self-motivated and directed with an attention to detail Good understanding of relational theory, data warehousing design techniques, data modeling, and data design best practices Ability to communicate well with multiple groups to elicit and confirm data requirements Collaborating with other AWS Services Teams Apply to Compass Group today! Click here to Learn More about the Compass Story Compass Group is an equal opportunity employer. At Compass, we are committed to treating all Applicants and Associates fairly based on their abilities, achievements, and experience without regard to race, national origin, sex, age, disability, veteran status, sexual orientation, gender identity, or any other classification protected by law. Qualified candidates must be able to perform the essential functions of this position satisfactorily with or without a reasonable accommodation. Disclaimer: this job post is not necessarily an exhaustive list of all essential responsibilities, skills, tasks, or requirements associated with this position. While this is intended to be an accurate reflection of the position posted, the Company reserves the right to modify or change the essential functions of the job based on business necessity. We will consider for employment all qualified applicants, including those with a criminal history (including relevant driving history), in a manner consistent with all applicable federal, state, and local laws, including the City of Los Angeles’ Fair Chance Initiative for Hiring Ordinance, the San Francisco Fair Chance Ordinance, and the New York Fair Chance Act. We encourage applicants with a criminal history (and driving history) to apply. Compass Corporate maintains a drug-free workplace. Associates at Corporate are offered many fantastic benefits. Medical Dental Vision Life Insurance/ AD Disability Insurance Retirement Plan Paid Time Off Holiday Time Off (varies by site/state) Associate Shopping Program Health and Wellness Programs Discount Marketplace Identity Theft Protection Pet Insurance Commuter Benefits Employee Assistance Program Flexible Spending Accounts (FSAs) Req ID: 1251316 Compass Corporate Michelle Lombardozzi [[req_classification]]",Competitive,2024-01-14
Amgen,"HOW MIGHT YOU DEFY IMAGINATION ? You’ve worked hard to become the professional you are today and are now ready to take the next step in your career. How will you put your skills, experience and passion to work toward your goals? At Amgen, our shared mission—to serve patients—drives all that we do. It is key to our becoming one of the world’s leading biotechnology companies, reaching over 10 million patients worldwide. Come do your best work alongside other innovative, driven professionals in this meaningful role. Senior Data Engineer Live What you will do Let’s do this. Let’s change the world. In this vital role you will be designing and developing robust data models, data pipelines and data products as part of a product team of data scientists, business analysts, and software engineers. The team will rely on your expertise in automating the transformation and manipulation of data to generate insights for QC laboratories across Amgen. Responsibilities Determine which tools and architectures are available for pipeline development based on the data source format and current technologies Develop data flow pipelines to extract, transform, and load data from various data sources in various forms, including custom ETL pipelines that enable model and product development Build data products and service processes which perform data transformation, metadata extraction, workload management and error processing management to ensure high quality data Collaborate with Data Scientists to normalize and clean data in order to produce robust and high-quality data assets Support Data Science team with model design by anticipating and engineering features needed to drive model development and insight generation Provide clear documentation for delivered solutions and processes, integrating documentation Ensure that data engineering applications are aligned with the overall architectural and development guidelines Identify and implement internal process improvements for data management (automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability) Partner with business stakeholders and other technical team members to identify and acquire data sources that are most relevant to business needs and goals Demonstrate deep technical and domain knowledge in order to propose potential solutions Mentor junior data engineers on best practices in the industry and in the Amgen data landscape Win What we expect of you We are all different, yet we all use our unique contributions to serve patients. The professional we seek will have these qualifications. Basic Qualifications: Doctorate degree OR Master’s degree and 3 years of Information Systems experience OR Bachelor’s degree and 5 years of Information Systems experience OR Associate degree and 10 years of Information Systems experience Or High school diploma / GED and 12 years of Information Systems experience Preferred Qualifications: Hands on experience writing SQL using any RDBMS (Redshift, Postgres, MySQL, Teradata, Oracle, etc.). Proficient in one of the coding languages (Python, Java, Scala) Experience with Spark, Hive, Kafka, Kinesis, Spark Streaming, and Airflow. Hands on experience using Databricks/Jupyter or similar notebook environment. Preferred Qualifications: Experience working with teams of data scientists, software engineers and business experts to drive insights Strong understanding of relevant data standards and industry trends Ability to understand new business requirements and prioritize them for delivery Experience working in biopharma/life sciences industry Experience with Schema Design & Dimensional data modeling. Experience with AWS Services such as EC2, S3, Redshift/Spectrum, Glue, Athena, RDS, Lambda, and API gateway. Experience with software DevOps CI/CD tools, such Git, Jenkins, Linux, and Shell Script Experience working in an agile environment (i.e. user stories, iterative development, etc.) Experience working with test-driven development and software test automation Experience working in a Product Team environment Thrive What you can expect of us As we work to develop treatments that take care of others, we also work to care for our teammates’ professional and personal growth and well-being. Amgen offers a Total Rewards Plan comprising health and welfare plans for staff and eligible dependents, financial plans with opportunities to save towards retirement or other goals, work/life balance, and career development opportunities including: Comprehensive employee benefits package, including a Retirement and Savings Plan with generous company contributions, group medical, dental and vision coverage, life and disability insurance, and flexible spending accounts. A discretionary annual bonus program, or for field sales representatives, a sales-based incentive plan Stock-based long-term incentives Award-winning time-off plans and bi-annual company-wide shutdowns Flexible work models, including remote work arrangements, where possible Apply now for a career that defies imagination Objects in your future are closer than they appear. Join us. careers.amgen.com Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",Senior Data Engineer,"Remote in Thousand Oaks, CA","HOW MIGHT YOU DEFY IMAGINATION ? You’ve worked hard to become the professional you are today and are now ready to take the next step in your career. How will you put your skills, experience and passion to work toward your goals? At Amgen, our shared mission—to serve patients—drives all that we do. It is key to our becoming one of the world’s leading biotechnology companies, reaching over 10 million patients worldwide. Come do your best work alongside other innovative, driven professionals in this meaningful role. Senior Data Engineer Live What you will do Let’s do this. Let’s change the world. In this vital role you will be designing and developing robust data models, data pipelines and data products as part of a product team of data scientists, business analysts, and software engineers. The team will rely on your expertise in automating the transformation and manipulation of data to generate insights for QC laboratories across Amgen. Responsibilities Determine which tools and architectures are available for pipeline development based on the data source format and current technologies Develop data flow pipelines to extract, transform, and load data from various data sources in various forms, including custom ETL pipelines that enable model and product development Build data products and service processes which perform data transformation, metadata extraction, workload management and error processing management to ensure high quality data Collaborate with Data Scientists to normalize and clean data in order to produce robust and high-quality data assets Support Data Science team with model design by anticipating and engineering features needed to drive model development and insight generation Provide clear documentation for delivered solutions and processes, integrating documentation Ensure that data engineering applications are aligned with the overall architectural and development guidelines Identify and implement internal process improvements for data management (automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability) Partner with business stakeholders and other technical team members to identify and acquire data sources that are most relevant to business needs and goals Demonstrate deep technical and domain knowledge in order to propose potential solutions Mentor junior data engineers on best practices in the industry and in the Amgen data landscape Win What we expect of you We are all different, yet we all use our unique contributions to serve patients. The professional we seek will have these qualifications. Basic Qualifications: Doctorate degree OR Master’s degree and 3 years of Information Systems experience OR Bachelor’s degree and 5 years of Information Systems experience OR Associate degree and 10 years of Information Systems experience Or High school diploma / GED and 12 years of Information Systems experience Preferred Qualifications: Hands on experience writing SQL using any RDBMS (Redshift, Postgres, MySQL, Teradata, Oracle, etc.). Proficient in one of the coding languages (Python, Java, Scala) Experience with Spark, Hive, Kafka, Kinesis, Spark Streaming, and Airflow. Hands on experience using Databricks/Jupyter or similar notebook environment. Preferred Qualifications: Experience working with teams of data scientists, software engineers and business experts to drive insights Strong understanding of relevant data standards and industry trends Ability to understand new business requirements and prioritize them for delivery Experience working in biopharma/life sciences industry Experience with Schema Design & Dimensional data modeling. Experience with AWS Services such as EC2, S3, Redshift/Spectrum, Glue, Athena, RDS, Lambda, and API gateway. Experience with software DevOps CI/CD tools, such Git, Jenkins, Linux, and Shell Script Experience working in an agile environment (i.e. user stories, iterative development, etc.) Experience working with test-driven development and software test automation Experience working in a Product Team environment Thrive What you can expect of us As we work to develop treatments that take care of others, we also work to care for our teammates’ professional and personal growth and well-being. Amgen offers a Total Rewards Plan comprising health and welfare plans for staff and eligible dependents, financial plans with opportunities to save towards retirement or other goals, work/life balance, and career development opportunities including: Comprehensive employee benefits package, including a Retirement and Savings Plan with generous company contributions, group medical, dental and vision coverage, life and disability insurance, and flexible spending accounts. A discretionary annual bonus program, or for field sales representatives, a sales-based incentive plan Stock-based long-term incentives Award-winning time-off plans and bi-annual company-wide shutdowns Flexible work models, including remote work arrangements, where possible Apply now for a career that defies imagination Objects in your future are closer than they appear. Join us. careers.amgen.com Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.","$124,933 - $158,726 a year",2024-01-14
Optum,"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together. The Senior Data Services Engineer assists in the planning, design, and implementation of database system and all the methods, strategies, and development of Data Ingestion deliverables. As a Senior Data Services Engineer you will work closely with the business and our end users to capture requirements, perform volume analysis in order to develop and support the database platforms. Additionally, you will provide both formal and informal recommendations and guidance to the various business units on how best to integrate, automate, simplify, consolidate and develop systems and processes aligned with business goals and objectives. You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges. Primary Responsibilities: Work with subject matter experts to discover, define, assess, and clearly document business needs and requirements Provide oversight and audits of database systems and ETL processes Ability to assist in building consensus between multiple internal stakeholders with competing priorities Ensure technology solutions will meet business needs while maintaining efficiency and usability Assist in creation of data transformations, migration planning, cleansing, backups and archival controls Present complex technical business and technology subjects at appropriate detail for business users and executive-level audiences Build business requirements-driven test cases and test plans Provide support to business end users and enable other business analysts to leverage applications Troubleshoot and track end user data issues to be sure resolution is satisfactory for business users Serve as subject matter expert for business processes, data flows, and corresponding new business operations automation capabilities Build solid relationships with users to better understand their needs and requirements Provide effective and long-term solutions independently with minimal to no management input Produce deliverables such as database diagrams, functional system requirements and business process and data flows Provide ad hoc reporting on medical claim auditing activities as needed Describe complex system requirements and tests, including user stories and use cases You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in. Years of post-high school education can be substituted/is equivalent to years of experience. Required Qualifications: 3+ years serving as a Data Analyst in a complex healthcare or medical industry application environment, with specific project experience in one or more of the following three areas: medical claims auditing, data warehousing, business process automation technology, analytic/business intelligence tools Demonstrated expertise with Windows platforms, SQL (MS-SQL, Postgres, etc.), Reporting packages (SSRS, Crystal, etc.), ETL applications Proven knowledge of building out ETL processes Proven foundational understanding of data importation using various products Proven knowledge of Relational Database Development Demonstrated thorough understanding of business processes Well-versed in data analysis; able to design/describe and communicate processes to both business and technology staff Experience working in data-driven, analytical medical claim auditing organization Ability to collaboratively work with others in responding in diagnosing, identifying and remediating issues and defects Proven familiarity with data structures based on analysis of medical claim auditing business processes Preferred Qualifications: Medical claims audit experience Hadoop experience All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy California, Colorado, Connecticut, Hawaii, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only: The salary range for California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island or Washington residents is $70,200 to $137,800 annually. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives. At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission. Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law. UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.",Senior Data Services Engineer - Remote,"Remote in Schaumburg, IL 60168","Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together. The Senior Data Services Engineer assists in the planning, design, and implementation of database system and all the methods, strategies, and development of Data Ingestion deliverables. As a Senior Data Services Engineer you will work closely with the business and our end users to capture requirements, perform volume analysis in order to develop and support the database platforms. Additionally, you will provide both formal and informal recommendations and guidance to the various business units on how best to integrate, automate, simplify, consolidate and develop systems and processes aligned with business goals and objectives. You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges. Primary Responsibilities: Work with subject matter experts to discover, define, assess, and clearly document business needs and requirements Provide oversight and audits of database systems and ETL processes Ability to assist in building consensus between multiple internal stakeholders with competing priorities Ensure technology solutions will meet business needs while maintaining efficiency and usability Assist in creation of data transformations, migration planning, cleansing, backups and archival controls Present complex technical business and technology subjects at appropriate detail for business users and executive-level audiences Build business requirements-driven test cases and test plans Provide support to business end users and enable other business analysts to leverage applications Troubleshoot and track end user data issues to be sure resolution is satisfactory for business users Serve as subject matter expert for business processes, data flows, and corresponding new business operations automation capabilities Build solid relationships with users to better understand their needs and requirements Provide effective and long-term solutions independently with minimal to no management input Produce deliverables such as database diagrams, functional system requirements and business process and data flows Provide ad hoc reporting on medical claim auditing activities as needed Describe complex system requirements and tests, including user stories and use cases You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in. Years of post-high school education can be substituted/is equivalent to years of experience. Required Qualifications: 3+ years serving as a Data Analyst in a complex healthcare or medical industry application environment, with specific project experience in one or more of the following three areas: medical claims auditing, data warehousing, business process automation technology, analytic/business intelligence tools Demonstrated expertise with Windows platforms, SQL (MS-SQL, Postgres, etc.), Reporting packages (SSRS, Crystal, etc.), ETL applications Proven knowledge of building out ETL processes Proven foundational understanding of data importation using various products Proven knowledge of Relational Database Development Demonstrated thorough understanding of business processes Well-versed in data analysis; able to design/describe and communicate processes to both business and technology staff Experience working in data-driven, analytical medical claim auditing organization Ability to collaboratively work with others in responding in diagnosing, identifying and remediating issues and defects Proven familiarity with data structures based on analysis of medical claim auditing business processes Preferred Qualifications: Medical claims audit experience Hadoop experience All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy California, Colorado, Connecticut, Hawaii, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only: The salary range for California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island or Washington residents is $70,200 to $137,800 annually. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives. At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission. Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law. UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.","$70,200 - $137,800 a year",2024-01-14
Michael Baker International,"JOB DESCRIPTION This position will be responsible for developing, maintaining, and optimizing our data warehouse, data pipeline, and data products. This position will strive for efficiency by aligning data systems with business goals. Essential Duties & Responsibilities Design, build and maintain batch or real-time data pipelines. Maintain and optimize the data infrastructure required for accurate extraction, transformation, and loading of data from a wide variety of data sources. Develop ETL (extract, transform, load) processes to help extract and manipulate data from multiple sources. Automate data workflows such as data ingestion, aggregation, and ETL processing. Prepare raw data in data lake into a consumable datasets for both technical and non-technical stakeholders. Build, maintain, and deploy data products for analytics and data science teams on cloud platforms (e.g. Azure). Ensure data accuracy, integrity, privacy, security, and compliance through quality control procedures. Monitor data systems performance and implement optimization strategies. Leverage data controls to maintain data privacy, security, compliance, and quality for allocated areas of ownership. Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization. Experience Minimum of 8 years of IT experience with at least 4 years of experience as Data Engineer Education Degree in Computer Science, IT, or similar field Minimum Qualifications, Skills and Abilities To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. Advanced SQL skills and experience with relational databases and database design Technical expertise with data models, data mining, and segmentation techniques Working knowledge of Cloud-based solutions (e.g., Azure) Experience building and deploying machine learning models Great numerical and analytical skills Excellent problem-solving, communication, and organizational skills Proven ability to work independently and with a team COMPENSATION The approximate compensation range for this position is $93,000 to $147,000. This compensation range is a good faith estimate for the position at the time of posting. Actual compensation is dependent upon factors such as education, qualifications, experience, skillset, and physical work location. ABOUT US Michael Baker International, a leading provider of engineering and consulting services, including design, planning, architectural, environmental, construction and program management, has been solving some of the world’s most complex infrastructure challenges for more than 80 years with a legacy of expertise, experience, innovation and integrity. Based in Pittsburgh and with nearly 100 offices nationwide, we partner with clients on everything from roads, bridges, tunnels, mass transit, and airports, to water treatment plants, arctic oil pipelines, environmental restoration and specialized overseas construction. We serve as a trusted adviser to the communities we serve, making them safer, more accessible, more sustainable and more prosperous. We provide visionary leadership in facilitating transformational change for our clients. Our work delivers differentiating innovations and dedicated experts who challenge the status quo and share a world of diverse experience and an impassioned entrepreneurial spirit. We deliver quality of life. We Make a Difference. ABOUT US Michael Baker International, a leading provider of engineering and consulting services, including design, planning, architectural, environmental, construction and program management, has been solving some of the world’s most complex infrastructure challenges for more than 80 years with a legacy of expertise, experience, innovation and integrity. Based in Pittsburgh and with nearly 100 offices nationwide, we partner with clients on everything from roads, bridges, tunnels, mass transit, and airports, to water treatment plants, arctic oil pipelines, environmental restoration and specialized overseas construction. We serve as a trusted adviser to the communities we serve, making them safer, more accessible, more sustainable and more prosperous. We provide visionary leadership in facilitating transformational change for our clients. Our work delivers differentiating innovations and dedicated experts who challenge the status quo and share a world of diverse experience and an impassioned entrepreneurial spirit. We deliver quality of life. We Make a Difference. Michael Baker International is proud to be an Affirmative Action/Equal Opportunity Employer. Michael Baker International provides equal employment opportunity for all persons, in all facets of employment. Michael Baker International maintains a drug-free workplace and performs pre-employment substance abuse testing and background checks. We encourage all qualified applicants to apply for any open position for which they feel they are qualified and all will receive consideration for employment without regard to race, color, religion, age, gender, sexual orientation, gender identity, national origin, citizenship status, marital status, genetic information, disability, protected veteran status or any other legally protected status. EEO is the Law. Applicants to and employees of Michael Baker International are protected under Federal law from discrimination.",Data Engineer - REMOTE,Remote in United States,"JOB DESCRIPTION This position will be responsible for developing, maintaining, and optimizing our data warehouse, data pipeline, and data products. This position will strive for efficiency by aligning data systems with business goals. Essential Duties & Responsibilities Design, build and maintain batch or real-time data pipelines. Maintain and optimize the data infrastructure required for accurate extraction, transformation, and loading of data from a wide variety of data sources. Develop ETL (extract, transform, load) processes to help extract and manipulate data from multiple sources. Automate data workflows such as data ingestion, aggregation, and ETL processing. Prepare raw data in data lake into a consumable datasets for both technical and non-technical stakeholders. Build, maintain, and deploy data products for analytics and data science teams on cloud platforms (e.g. Azure). Ensure data accuracy, integrity, privacy, security, and compliance through quality control procedures. Monitor data systems performance and implement optimization strategies. Leverage data controls to maintain data privacy, security, compliance, and quality for allocated areas of ownership. Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization. Experience Minimum of 8 years of IT experience with at least 4 years of experience as Data Engineer Education Degree in Computer Science, IT, or similar field Minimum Qualifications, Skills and Abilities To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. Advanced SQL skills and experience with relational databases and database design Technical expertise with data models, data mining, and segmentation techniques Working knowledge of Cloud-based solutions (e.g., Azure) Experience building and deploying machine learning models Great numerical and analytical skills Excellent problem-solving, communication, and organizational skills Proven ability to work independently and with a team COMPENSATION The approximate compensation range for this position is $93,000 to $147,000. This compensation range is a good faith estimate for the position at the time of posting. Actual compensation is dependent upon factors such as education, qualifications, experience, skillset, and physical work location. ABOUT US Michael Baker International, a leading provider of engineering and consulting services, including design, planning, architectural, environmental, construction and program management, has been solving some of the world’s most complex infrastructure challenges for more than 80 years with a legacy of expertise, experience, innovation and integrity. Based in Pittsburgh and with nearly 100 offices nationwide, we partner with clients on everything from roads, bridges, tunnels, mass transit, and airports, to water treatment plants, arctic oil pipelines, environmental restoration and specialized overseas construction. We serve as a trusted adviser to the communities we serve, making them safer, more accessible, more sustainable and more prosperous. We provide visionary leadership in facilitating transformational change for our clients. Our work delivers differentiating innovations and dedicated experts who challenge the status quo and share a world of diverse experience and an impassioned entrepreneurial spirit. We deliver quality of life. We Make a Difference. ABOUT US Michael Baker International, a leading provider of engineering and consulting services, including design, planning, architectural, environmental, construction and program management, has been solving some of the world’s most complex infrastructure challenges for more than 80 years with a legacy of expertise, experience, innovation and integrity. Based in Pittsburgh and with nearly 100 offices nationwide, we partner with clients on everything from roads, bridges, tunnels, mass transit, and airports, to water treatment plants, arctic oil pipelines, environmental restoration and specialized overseas construction. We serve as a trusted adviser to the communities we serve, making them safer, more accessible, more sustainable and more prosperous. We provide visionary leadership in facilitating transformational change for our clients. Our work delivers differentiating innovations and dedicated experts who challenge the status quo and share a world of diverse experience and an impassioned entrepreneurial spirit. We deliver quality of life. We Make a Difference. Michael Baker International is proud to be an Affirmative Action/Equal Opportunity Employer. Michael Baker International provides equal employment opportunity for all persons, in all facets of employment. Michael Baker International maintains a drug-free workplace and performs pre-employment substance abuse testing and background checks. We encourage all qualified applicants to apply for any open position for which they feel they are qualified and all will receive consideration for employment without regard to race, color, religion, age, gender, sexual orientation, gender identity, national origin, citizenship status, marital status, genetic information, disability, protected veteran status or any other legally protected status. EEO is the Law. Applicants to and employees of Michael Baker International are protected under Federal law from discrimination.","$93,000 - $147,000 a year",2024-01-14
RELX,"About the business: LexisNexis Risk Solutions is the essential partner in the assessment of risk. Within our Business Services vertical, we help our customers solve difficult problems in the areas of Anti-Money Laundering/Counter Terrorist Financing, Identity Authentication & Verification, Fraud and Credit Risk mitigation and Customer Data Management. We are a fast-growing division of the RELX Group, and the convergence of many industry-leading solutions are providing LexisNexis Risk with significant opportunity for future growth. You can learn more about LexisNexis Risk at the link below. https://risk.lexisnexis.com/ About our team: This Data Engineering role is within our Enterprise Data Intelligence team which is focused on providing broad support across LexisNexis Risk for data analytics and business intelligence. The role will deliver solutions across multiple stakeholder groups utilizing various datasets within our business. About the job: As a Data Engineer IV within our squad, you will play a pivotal role in driving our data engineering initiatives, ensuring data reliability, and contributing to the success of our projects. You will be responsible for: Leading complex data integration tasks to ensure smooth data flows. Collaborating with squad members and stakeholders to define project requirements. Developing efficient and clean code adhering to best practices. Creating and maintaining detailed specifications for data components. Addressing complex data engineering issues and bugs, employing root cause analysis. Identifying and implementing opportunities for automation and process improvements. Applying data modeling and ETL expertise to shape data structures effectively. Mentoring and providing guidance to junior data engineers. Staying updated on emerging data engineering technologies and trends. Ensuring compliance with data quality standards, best practices, and data security measures. Qualifications: Proficiency in agile software methodologies Strong expertise in data manipulation languages and optimization techniques. In-depth knowledge of normalized/dimensional data modeling principles. Ability in multiple data storage subsystems. Mastery of development languages including SQL, HPCC ECL, Python, PySpark, and others Ability to work effectively with internal and external technology resources. Knowledge of test-driven development. Proficiency in Git for central code management. Experience querying MySQL, SQL Server, or Azure Synapse databases. Understanding and ability to parse XML blob data. Proficiency in automation and efficiency-enhancing tools such as Azure Data Factory. Exceptional problem-solving, research, attention to detail, and communication skills. Women in tech: LexisNexis Risk Solutions is supportive of women in Technology and has been a founding signature for the Tech Talent Charter. We have the following initiatives in place to support women in technology: Mentoring scheme for women in technology, Women’s network forum, regularly run events for schools about careers in technology to inspire the next generation of girls in technology. Learn more about the LexisNexis risk team and how we work here #LI-MR1 #LI-HYBRID At LexisNexis Risk Solutions, having diverse employees with different perspectives is key to creating innovative new products for our global customers. We have 30 diversity employee networks globally and prioritize inclusive leadership and equitable processes as part of our culture. Our aim is for every employee to be the best version of themselves. We would actively welcome applications from candidates of diverse backgrounds and underrepresented groups. We are an equal opportunity employer: qualified applicants are considered for and treated during employment without regard to race, color, creed, religion, sex, national origin, citizenship status, disability status, protected veteran status, age, marital status, sexual orientation, gender identity, genetic information, or any other characteristic protected by law. We are committed to providing a fair and accessible hiring process. If you have a disability or other need that requires accommodation or adjustment, please let us know by completing our Applicant Request Support Form: https://forms.office.com/r/eVgFxjLmAK . Please read our Candidate Privacy Policy .",Data Engineer IV,"Hybrid remote in Alpharetta, GA","About the business: LexisNexis Risk Solutions is the essential partner in the assessment of risk. Within our Business Services vertical, we help our customers solve difficult problems in the areas of Anti-Money Laundering/Counter Terrorist Financing, Identity Authentication & Verification, Fraud and Credit Risk mitigation and Customer Data Management. We are a fast-growing division of the RELX Group, and the convergence of many industry-leading solutions are providing LexisNexis Risk with significant opportunity for future growth. You can learn more about LexisNexis Risk at the link below. https://risk.lexisnexis.com/ About our team: This Data Engineering role is within our Enterprise Data Intelligence team which is focused on providing broad support across LexisNexis Risk for data analytics and business intelligence. The role will deliver solutions across multiple stakeholder groups utilizing various datasets within our business. About the job: As a Data Engineer IV within our squad, you will play a pivotal role in driving our data engineering initiatives, ensuring data reliability, and contributing to the success of our projects. You will be responsible for: Leading complex data integration tasks to ensure smooth data flows. Collaborating with squad members and stakeholders to define project requirements. Developing efficient and clean code adhering to best practices. Creating and maintaining detailed specifications for data components. Addressing complex data engineering issues and bugs, employing root cause analysis. Identifying and implementing opportunities for automation and process improvements. Applying data modeling and ETL expertise to shape data structures effectively. Mentoring and providing guidance to junior data engineers. Staying updated on emerging data engineering technologies and trends. Ensuring compliance with data quality standards, best practices, and data security measures. Qualifications: Proficiency in agile software methodologies Strong expertise in data manipulation languages and optimization techniques. In-depth knowledge of normalized/dimensional data modeling principles. Ability in multiple data storage subsystems. Mastery of development languages including SQL, HPCC ECL, Python, PySpark, and others Ability to work effectively with internal and external technology resources. Knowledge of test-driven development. Proficiency in Git for central code management. Experience querying MySQL, SQL Server, or Azure Synapse databases. Understanding and ability to parse XML blob data. Proficiency in automation and efficiency-enhancing tools such as Azure Data Factory. Exceptional problem-solving, research, attention to detail, and communication skills. Women in tech: LexisNexis Risk Solutions is supportive of women in Technology and has been a founding signature for the Tech Talent Charter. We have the following initiatives in place to support women in technology: Mentoring scheme for women in technology, Women’s network forum, regularly run events for schools about careers in technology to inspire the next generation of girls in technology. Learn more about the LexisNexis risk team and how we work here #LI-MR1 #LI-HYBRID At LexisNexis Risk Solutions, having diverse employees with different perspectives is key to creating innovative new products for our global customers. We have 30 diversity employee networks globally and prioritize inclusive leadership and equitable processes as part of our culture. Our aim is for every employee to be the best version of themselves. We would actively welcome applications from candidates of diverse backgrounds and underrepresented groups. We are an equal opportunity employer: qualified applicants are considered for and treated during employment without regard to race, color, creed, religion, sex, national origin, citizenship status, disability status, protected veteran status, age, marital status, sexual orientation, gender identity, genetic information, or any other characteristic protected by law. We are committed to providing a fair and accessible hiring process. If you have a disability or other need that requires accommodation or adjustment, please let us know by completing our Applicant Request Support Form: https://forms.office.com/r/eVgFxjLmAK . Please read our Candidate Privacy Policy .",Competitive,2024-01-14
BayOne,"Title: GCP Data Engineer Location: Dallas, TX / Remote Till Covid Long Term Contract Local Preferred Number of Positions: 4 Job Description & Responsibilities: Top Skills: As a Data engineer (successfully lead the delivery of projects including end-to-end requirements, data analysis, perform data engineeringactivities and work with the users for data validation. 5+ years of experience in Datawarehouse or Data Analytics 5+ years of Hands on experience in setting up Google Cloud Data Engineering 5+ years of Strong Hands on experience in any programming (Python, Spark) Professional Cloud Data Engineer Certification is a plus Requisite Abilities and/or Skills Perform requirement gathering with the business users and SME and build a plan Perform ETL and data engineering work by leveraging multiple google cloud components using Cloud Dataflow, Cloud Data Proc, Google BigQuery Knowledge on Data Modelling and reporting using Google Cloud BigQuery Knowledge on building data pipelines leveraging GCP best methodologies Strong understanding towards Kubernetes, docker containers and to deploy GCP services Experience in writing code to extract, transform data from multiple data sources including Experience in ETL tools like Informatica or any ETL tools Experience in scheduling like Airflow, Cloud Composer etc. Experience in CI/CD automation pipeline facilitating automated deployment and testing Excellent verbal and written communication, problem solving and interpersonal skills. Experience in data catalog and metadata management. Experience with JIRA or any other Project Management Tools Deliver end to end comprehensive documentation along with code samples Experience in one or multiple scripting languages / Cloud solutionsis a plus Key Accountabilities and Priorities: Build a and operationalize pipelines to include data acquisition, staging, integration of new data sources, cataloging, cleansing, batch and stream processing, transformation, and consumption Independently work on assigned projects and f oster a collaborative environment for a high-performing team Gather Business requirements, Review business priorities, Analyze options & risks Quickly Understand and formulate application level requirements pertaining to complex workflows, or integration with external applications Additional Information: Bachelor's Degree. Masters is a plus. 8+ years' experience in Information Technology and/or in IT Professional Services.",GCP Data Engineer,"Pleasanton, CA 94588","Title: GCP Data Engineer Location: Dallas, TX / Remote Till Covid Long Term Contract Local Preferred Number of Positions: 4 Job Description & Responsibilities: Top Skills: As a Data engineer (successfully lead the delivery of projects including end-to-end requirements, data analysis, perform data engineeringactivities and work with the users for data validation. 5+ years of experience in Datawarehouse or Data Analytics 5+ years of Hands on experience in setting up Google Cloud Data Engineering 5+ years of Strong Hands on experience in any programming (Python, Spark) Professional Cloud Data Engineer Certification is a plus Requisite Abilities and/or Skills Perform requirement gathering with the business users and SME and build a plan Perform ETL and data engineering work by leveraging multiple google cloud components using Cloud Dataflow, Cloud Data Proc, Google BigQuery Knowledge on Data Modelling and reporting using Google Cloud BigQuery Knowledge on building data pipelines leveraging GCP best methodologies Strong understanding towards Kubernetes, docker containers and to deploy GCP services Experience in writing code to extract, transform data from multiple data sources including Experience in ETL tools like Informatica or any ETL tools Experience in scheduling like Airflow, Cloud Composer etc. Experience in CI/CD automation pipeline facilitating automated deployment and testing Excellent verbal and written communication, problem solving and interpersonal skills. Experience in data catalog and metadata management. Experience with JIRA or any other Project Management Tools Deliver end to end comprehensive documentation along with code samples Experience in one or multiple scripting languages / Cloud solutionsis a plus Key Accountabilities and Priorities: Build a and operationalize pipelines to include data acquisition, staging, integration of new data sources, cataloging, cleansing, batch and stream processing, transformation, and consumption Independently work on assigned projects and f oster a collaborative environment for a high-performing team Gather Business requirements, Review business priorities, Analyze options & risks Quickly Understand and formulate application level requirements pertaining to complex workflows, or integration with external applications Additional Information: Bachelor's Degree. Masters is a plus. 8+ years' experience in Information Technology and/or in IT Professional Services.",Competitive,2024-01-14
Sumitomo Mitsui Banking Corporation,"SMBC Group is a top-tier global financial group. Headquartered in Tokyo and with a 400-year history, SMBC Group offers a diverse range of financial services, including banking, leasing, securities, credit cards, and consumer finance. The Group has more than 130 offices and 80,000 employees worldwide in nearly 40 countries. Sumitomo Mitsui Financial Group, Inc. (SMFG) is the holding company of SMBC Group, which is one of the three largest banking groups in Japan. SMFG’s shares trade on the Tokyo, Nagoya, and New York (NYSE: SMFG) stock exchanges. In the Americas, SMBC Group has a presence in the US, Canada, Mexico, Brazil, Chile, Colombia, and Peru. Backed by the capital strength of SMBC Group and the value of its relationships in Asia, the Group offers a range of commercial and investment banking services to its corporate, institutional, and municipal clients. It connects a diverse client base to local markets and the organization’s extensive global network. The Group’s operating companies in the Americas include Sumitomo Mitsui Banking Corp. (SMBC), SMBC Nikko Securities America, Inc., SMBC Capital Markets, Inc., SMBC Rail Services LLC, Manufacturers Bank, JRI America, Inc., SMBC Leasing and Finance, Inc., Banco Sumitomo Mitsui Brasileiro S.A., and Sumitomo Mitsui Finance and Leasing Co., Ltd. The anticipated salary range for this role is between $75,000.00 and $150,000.00. The specific salary offered to an applicant will be based on their individual qualifications, experiences, and an analysis of the current compensation paid in their geography and the market for similar roles at the time of hire. The role may also be eligible for an annual discretionary incentive award. In addition to cash compensation, SMBC offers a competitive portfolio of benefits to its employees. Role Description SMBC is in the process of leading a Digital Transformation across our Americas Division as we continue to modernize our technology, focus on our data-driven approach, grow, and plan for the future. As a result of this expansion, we are seeking an experienced software engineer, with 3-7 years of experience to support the design and development of a strategic data platform for SMBC Capital Markets and Nikko Securities Group. Role Objectives This role will be part of the data strategy team spanning across the SMBC Capital Markets and Nikko securities teams, SMBC Americas Division’s broker-dealer and swap-dealer entities This role will be involved in the active development of the data platform, beginning with the establishment of a reference data system for securities and pricing data, and later moving to other data domains This role will be part of the SMBC CM/Nikko development team, and will need to follow internal developments standards to contribute to the overall agenda of the team Qualifications and Skills 3-7 years of experience in a large & complex application development environment, preferably in the financial services industry Basic understanding of equities and fixed-income instruments Strong SQL skills and experience with RDMS 3+ years of experience programming in Python and Java, Experience with the DevOps lifecycle (git, Jenkins, etc.) is preferred Experience with Jira/Confluence Experience with REST web services and microservice architecture Good understanding of ETL/ELT Experience with Cloud solutions (AWS or Azure) is preferred Additional Requirements SMBC’s employees participate in a hybrid workforce model that provides employees with an opportunity to work from home, as well as, from an SMBC office. SMBC requires that employees live within a reasonable commuting distance of their office location. Prospective candidates will learn more about their specific hybrid work schedule during their interview process. SMBC is an EO employer – M/F/Veteran/Disability. SMBC provides reasonable accommodations for employees and applicants with disabilities consistent with applicable law. If you need reasonable accommodation during the application process, please let us know at accommodations@smbcgroup.com. SMBC’s employees participate in a hybrid workforce model that provides employees with an opportunity to work from home, as well as, from an SMBC office. SMBC requires that employees live within a reasonable commuting distance of their office location. Prospective candidates will learn more about their specific hybrid work schedule during their interview process. We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, national origin, disability status, protected veteran status or any other characteristic protected by law. SMBC provides reasonable accommodations for employees and applicants with disabilities consistent with applicable law. If you need a reasonable accommodation during the application process, please let us know at accommodations@smbcgroup.com.",Data Engineer - Associate,"Hybrid remote in New York, NY 10172","SMBC Group is a top-tier global financial group. Headquartered in Tokyo and with a 400-year history, SMBC Group offers a diverse range of financial services, including banking, leasing, securities, credit cards, and consumer finance. The Group has more than 130 offices and 80,000 employees worldwide in nearly 40 countries. Sumitomo Mitsui Financial Group, Inc. (SMFG) is the holding company of SMBC Group, which is one of the three largest banking groups in Japan. SMFG’s shares trade on the Tokyo, Nagoya, and New York (NYSE: SMFG) stock exchanges. In the Americas, SMBC Group has a presence in the US, Canada, Mexico, Brazil, Chile, Colombia, and Peru. Backed by the capital strength of SMBC Group and the value of its relationships in Asia, the Group offers a range of commercial and investment banking services to its corporate, institutional, and municipal clients. It connects a diverse client base to local markets and the organization’s extensive global network. The Group’s operating companies in the Americas include Sumitomo Mitsui Banking Corp. (SMBC), SMBC Nikko Securities America, Inc., SMBC Capital Markets, Inc., SMBC Rail Services LLC, Manufacturers Bank, JRI America, Inc., SMBC Leasing and Finance, Inc., Banco Sumitomo Mitsui Brasileiro S.A., and Sumitomo Mitsui Finance and Leasing Co., Ltd. The anticipated salary range for this role is between $75,000.00 and $150,000.00. The specific salary offered to an applicant will be based on their individual qualifications, experiences, and an analysis of the current compensation paid in their geography and the market for similar roles at the time of hire. The role may also be eligible for an annual discretionary incentive award. In addition to cash compensation, SMBC offers a competitive portfolio of benefits to its employees. Role Description SMBC is in the process of leading a Digital Transformation across our Americas Division as we continue to modernize our technology, focus on our data-driven approach, grow, and plan for the future. As a result of this expansion, we are seeking an experienced software engineer, with 3-7 years of experience to support the design and development of a strategic data platform for SMBC Capital Markets and Nikko Securities Group. Role Objectives This role will be part of the data strategy team spanning across the SMBC Capital Markets and Nikko securities teams, SMBC Americas Division’s broker-dealer and swap-dealer entities This role will be involved in the active development of the data platform, beginning with the establishment of a reference data system for securities and pricing data, and later moving to other data domains This role will be part of the SMBC CM/Nikko development team, and will need to follow internal developments standards to contribute to the overall agenda of the team Qualifications and Skills 3-7 years of experience in a large & complex application development environment, preferably in the financial services industry Basic understanding of equities and fixed-income instruments Strong SQL skills and experience with RDMS 3+ years of experience programming in Python and Java, Experience with the DevOps lifecycle (git, Jenkins, etc.) is preferred Experience with Jira/Confluence Experience with REST web services and microservice architecture Good understanding of ETL/ELT Experience with Cloud solutions (AWS or Azure) is preferred Additional Requirements SMBC’s employees participate in a hybrid workforce model that provides employees with an opportunity to work from home, as well as, from an SMBC office. SMBC requires that employees live within a reasonable commuting distance of their office location. Prospective candidates will learn more about their specific hybrid work schedule during their interview process. SMBC is an EO employer – M/F/Veteran/Disability. SMBC provides reasonable accommodations for employees and applicants with disabilities consistent with applicable law. If you need reasonable accommodation during the application process, please let us know at accommodations@smbcgroup.com. SMBC’s employees participate in a hybrid workforce model that provides employees with an opportunity to work from home, as well as, from an SMBC office. SMBC requires that employees live within a reasonable commuting distance of their office location. Prospective candidates will learn more about their specific hybrid work schedule during their interview process. We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, national origin, disability status, protected veteran status or any other characteristic protected by law. SMBC provides reasonable accommodations for employees and applicants with disabilities consistent with applicable law. If you need a reasonable accommodation during the application process, please let us know at accommodations@smbcgroup.com.","$75,000 - $150,000 a year",2024-01-14
Crackajack Solutions,Remote Contract Opened 9 months ago Job Description Data Engineer (with Healthcare experience) Required Skills: SQL Databricks data engineering Snowflake data engineering QA experience for ETL experienced with data acquisition and ingestion using API and/or batch channels Experience with Healthcare,Data Engineer,Remote,Remote Contract Opened 9 months ago Job Description Data Engineer (with Healthcare experience) Required Skills: SQL Databricks data engineering Snowflake data engineering QA experience for ETL experienced with data acquisition and ingestion using API and/or batch channels Experience with Healthcare,Competitive,2024-01-14
BXGI,"Overview We are conducting a search for a Data Engineer to join our client’s engineering team in Palo Alto, CA or remotely anywhere in the United States. Interested in shaping the future of pediatrics behavioral health? Our client is looking for a skilled Data Engineer to join their Engineering team who has in-depth experience with a variety of databases and maintaining data repositories from multiple data sources using different schemas. Come join a world-class company delivering AI-powered diagnostics and therapeutics to dramatically improve the outlook of young children with cognitive and behavioral conditions. Ability to work remotely is possible, with quarterly visits to Palo Alto. Responsibilities Create and maintain a data repository for analytics by combining multiple data sources using different schemas. Work with various groups in Engineering to source the necessary data. Implement industry best practices including quality assurance and compliance standards related to Protected Health Information (HIPAA). Develop and maintain ETL processes to populate the data warehouse. Deploy utility applications to Amazon AWS EC2, Fargate or Kubernetes. Qualifications BS in Computer Science, Engineering or related discipline. 4+ years experience with two of MySQL, Oracle SQL, Postgres. Deep understanding of relational database structure and performance. Past experience maintaining data repositories with disparate data sources. Past experience building visualizations of underlying data. Experience with Linux, Python, REST API. Excellent verbal and written communication skills. Excellent organizational skills and attention to detail. Nice to Have Experience with Amazon Web Services (RDS) Experience building/supporting HIPAA-compliant software. Understanding of underlying technologies of Tableau (Hyper databases, Hyper API, server extracts, prep flows, Python libraries. Deep understanding of dimensional data warehouses Experience with design star and snowflake schemas Ability to come into the office every so often (once we actually can). If interested, please send your resume to lana@bxgi.com.",Data Engineer,"Remote in Palo Alto, CA","Overview We are conducting a search for a Data Engineer to join our client’s engineering team in Palo Alto, CA or remotely anywhere in the United States. Interested in shaping the future of pediatrics behavioral health? Our client is looking for a skilled Data Engineer to join their Engineering team who has in-depth experience with a variety of databases and maintaining data repositories from multiple data sources using different schemas. Come join a world-class company delivering AI-powered diagnostics and therapeutics to dramatically improve the outlook of young children with cognitive and behavioral conditions. Ability to work remotely is possible, with quarterly visits to Palo Alto. Responsibilities Create and maintain a data repository for analytics by combining multiple data sources using different schemas. Work with various groups in Engineering to source the necessary data. Implement industry best practices including quality assurance and compliance standards related to Protected Health Information (HIPAA). Develop and maintain ETL processes to populate the data warehouse. Deploy utility applications to Amazon AWS EC2, Fargate or Kubernetes. Qualifications BS in Computer Science, Engineering or related discipline. 4+ years experience with two of MySQL, Oracle SQL, Postgres. Deep understanding of relational database structure and performance. Past experience maintaining data repositories with disparate data sources. Past experience building visualizations of underlying data. Experience with Linux, Python, REST API. Excellent verbal and written communication skills. Excellent organizational skills and attention to detail. Nice to Have Experience with Amazon Web Services (RDS) Experience building/supporting HIPAA-compliant software. Understanding of underlying technologies of Tableau (Hyper databases, Hyper API, server extracts, prep flows, Python libraries. Deep understanding of dimensional data warehouses Experience with design star and snowflake schemas Ability to come into the office every so often (once we actually can). If interested, please send your resume to lana@bxgi.com.",Competitive,2024-01-14
Avenue Code,"Sobre a empresa: A Avenue Code é líder em consultoria de software e focada em oferecer soluções de desenvolvimento de ponta a ponta para a transformação digital em todas as verticais de organizações de diferentes setores. Somos uma empresa privada, lucrativa e, há 15 anos, estamos em uma trajetória contínua de crescimento. Preocupamo-nos com nossos clientes, parceiros e nosso time global. Preferimos a palavra “parceiro” a “fornecedor” e nosso investimento em relacionamentos profissionais é um reflexo dessa filosofia. Temos orgulho de nossa perspicácia técnica, de nossa capacidade colaborativa na resolução de problemas e do profissionalismo de nosso time #OneAvenueCode. Mais de 3 anos trabalhando em estratégias de diversidade e inclusão, a Avenue Code acredita que reconhecer a diferença e promover um lugar seguro, oportunidades de trabalho, representatividade e apoio é a melhor forma de fazer a transformação social acontecer. Todas as vagas de emprego visam promover o reconhecimento dos pilares da diversidade. Sobre a oportunidade: We are looking for a passionate, hard-working, and talented Data Engineer to join our team. You will be working with one of our biggest clients on products with a high impact on their revenue. Responsabilidades: You will architect, develop, and test large scale data solutions, to provide efficient analytical and reporting capabilities across global and regional sales and finance teams. You will develop highly scalable data pipelines to load data from various source systems, use Apache Airflow to orchestrate, schedule and monitor the workflows. Build generic and reusable solutions that can scale and utilize various technologies and frameworks to solve our complex business requirements. You will be required to understand existing solutions, fine-tune them and support them as needed. Qualificações necessárias: We know that each person has gone through different career paths that have provided further knowledge. If you don’t check all the points: go ahead! Apply anyways! Your experience is much more than a technology checklist. Strong expertise in dimensional modeling and data warehousing. Design and development experience with Cloud Data warehouses like Snowflake, Redshift, BigQuery etc. Hands on experience with Big-Data platforms like Spark, Dremio, Hadoop, MapReduce, Hive etc. Proficiency in design and development of custom ETL pipelines using SQL and scripting languages (Python/ Shell/ Golang) and workflow management tools like Airflow. Proficiency in advanced SQL, performance tuning. Experience with cloud computing platforms like AWS or Google Cloud. Será um diferencial possuir: More than 3 years of working on diversity and inclusion strategies, Avenue Code still believes that recognizing the difference and promoting a safe place, job opportunities, representativeness, and support is the best way to make the social transformation happen. All job openings aim to promote employment and recognition of diversity pillars. Avenue Code discloses salary range information in compliance with state and local pay transparency obligations. It considers a wide range of factors such as internal equity, geographic location, relevant education, qualifications, certifications, experience, skills, seniority, business or organizational needs and others. At Avenue Code it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range for US Based Data Engineer is 110 K to 200 K per year. #Li-Remote",Data Engineer,Remote in United States,"Sobre a empresa: A Avenue Code é líder em consultoria de software e focada em oferecer soluções de desenvolvimento de ponta a ponta para a transformação digital em todas as verticais de organizações de diferentes setores. Somos uma empresa privada, lucrativa e, há 15 anos, estamos em uma trajetória contínua de crescimento. Preocupamo-nos com nossos clientes, parceiros e nosso time global. Preferimos a palavra “parceiro” a “fornecedor” e nosso investimento em relacionamentos profissionais é um reflexo dessa filosofia. Temos orgulho de nossa perspicácia técnica, de nossa capacidade colaborativa na resolução de problemas e do profissionalismo de nosso time #OneAvenueCode. Mais de 3 anos trabalhando em estratégias de diversidade e inclusão, a Avenue Code acredita que reconhecer a diferença e promover um lugar seguro, oportunidades de trabalho, representatividade e apoio é a melhor forma de fazer a transformação social acontecer. Todas as vagas de emprego visam promover o reconhecimento dos pilares da diversidade. Sobre a oportunidade: We are looking for a passionate, hard-working, and talented Data Engineer to join our team. You will be working with one of our biggest clients on products with a high impact on their revenue. Responsabilidades: You will architect, develop, and test large scale data solutions, to provide efficient analytical and reporting capabilities across global and regional sales and finance teams. You will develop highly scalable data pipelines to load data from various source systems, use Apache Airflow to orchestrate, schedule and monitor the workflows. Build generic and reusable solutions that can scale and utilize various technologies and frameworks to solve our complex business requirements. You will be required to understand existing solutions, fine-tune them and support them as needed. Qualificações necessárias: We know that each person has gone through different career paths that have provided further knowledge. If you don’t check all the points: go ahead! Apply anyways! Your experience is much more than a technology checklist. Strong expertise in dimensional modeling and data warehousing. Design and development experience with Cloud Data warehouses like Snowflake, Redshift, BigQuery etc. Hands on experience with Big-Data platforms like Spark, Dremio, Hadoop, MapReduce, Hive etc. Proficiency in design and development of custom ETL pipelines using SQL and scripting languages (Python/ Shell/ Golang) and workflow management tools like Airflow. Proficiency in advanced SQL, performance tuning. Experience with cloud computing platforms like AWS or Google Cloud. Será um diferencial possuir: More than 3 years of working on diversity and inclusion strategies, Avenue Code still believes that recognizing the difference and promoting a safe place, job opportunities, representativeness, and support is the best way to make the social transformation happen. All job openings aim to promote employment and recognition of diversity pillars. Avenue Code discloses salary range information in compliance with state and local pay transparency obligations. It considers a wide range of factors such as internal equity, geographic location, relevant education, qualifications, certifications, experience, skills, seniority, business or organizational needs and others. At Avenue Code it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range for US Based Data Engineer is 110 K to 200 K per year. #Li-Remote",$110 an hour,2024-01-14
Frost Bank,"",DATA ENGINEER III,"San Antonio, TX","",Competitive,2024-01-14
MedWatchers,"",Data Engineer,"San Diego, CA","",Competitive,2024-01-14
PitchBook Data,"",Data Engineer,"Seattle, WA","","$90,800 - $130,000 a year",2024-01-14
CVS Health,"",Senior Data Engineer,"Irving, TX","","$90,000 - $180,000 a year",2024-01-14
Wise Equation Solutions,"",Data Engineer,"Melissa, TX 75454","",Competitive,2024-01-14
Providence,"",Health Care Data Engineer I IS,Remote in Texas,"",Competitive,2024-01-14
CVS Health,"",Sr Data Engineer - Clinical Data Repository,"Irving, TX","","$90,000 - $196,000 a year",2024-01-14
University Bank,"",Data Engineer,United States,"","$85,000 - $115,000 a year",2024-01-14
the NBA,"",Data Engineer - Snowflake,"Secaucus, NJ 07094","",Competitive,2024-01-14
Dana-Farber Cancer Institute,"",Data Engineer,"Boston, MA 02215 (Kenmore area)","",Competitive,2024-01-14
Agama Solutions,"Qualifications : Bachelor or Master's degree in Computer Science, Engineering, Information Systems or relevant degree is required 5+ years of professional work experience designing and implementing data pipelines in on-prem and cloud environments ( S3, EKS ) Experience with SQL/Relational databases. Experience with manipulating structured and unstructured data. Experience with distributed data systems such as Hadoop and related technologies (Spark, Trino, etc.). Background in both programming languages (Python & Scala ). Experience working with databases that power APIs for front-end applications. Experience with modern schedulers ( Airflow ) Responsibilities Support, Design, develop, test, deploy, maintain and improve data pipeline Designing and developing data processing techniques: automating manual processes, data delivery, data validation, data quality and integrity Communicate effectively with customers / team members & help with site up challenges. Must have skills: Python SQL Spark Scala AWS ecosystem ( S3, EKS ) Airflow Kafka Catalog store ( hive or similar ) Nice to have skills: Iceberg",Data Engineer III,United States,"Qualifications : Bachelor or Master's degree in Computer Science, Engineering, Information Systems or relevant degree is required 5+ years of professional work experience designing and implementing data pipelines in on-prem and cloud environments ( S3, EKS ) Experience with SQL/Relational databases. Experience with manipulating structured and unstructured data. Experience with distributed data systems such as Hadoop and related technologies (Spark, Trino, etc.). Background in both programming languages (Python & Scala ). Experience working with databases that power APIs for front-end applications. Experience with modern schedulers ( Airflow ) Responsibilities Support, Design, develop, test, deploy, maintain and improve data pipeline Designing and developing data processing techniques: automating manual processes, data delivery, data validation, data quality and integrity Communicate effectively with customers / team members & help with site up challenges. Must have skills: Python SQL Spark Scala AWS ecosystem ( S3, EKS ) Airflow Kafka Catalog store ( hive or similar ) Nice to have skills: Iceberg",Competitive,2024-01-14
HealthPartners/GHI,"JOB DESCRIPTION SENIOR DATA ENGINEER (Multiple Positions) – BLOOMINGTON, MN. Responsible for building, managing, and optimizing data pipelines that facilitate data movement in service of these goals by implementing and testing methods or build systems that improve data reliability and quality. Duties include: work with stakeholders, data scientists and analysts to frame problems, clean and integrate data, and determine best way to provision data on demand; collaborate with developers to design technology solutions that achieve measurable results at scale; help design and develop scalable, efficient data pipeline processes to handle data ingestion, cleansing, transformation, integration, and validation required to provide access to prepared data sets for analysts and data scientists; utilize development best practices including technical design reviews, implementing test plans, monitoring/alerting, peer code reviews, and documentation; collaborate with cross functional team to resolve data quality and operational issues and ensure timely delivery of products; incorporate core data management competencies including data governance, data security and data quality; participate in requirements gathering sessions to distill technical requirement from business requests; and, identify, design, and implement internal process improvements. Reqs. Master’s degree in Computer Science, Engineering, or closely related technical field, or, Bachelor’s degree in Computer Science, Engineering, or closely related technical field plus 2 years’ experience with hands-on data engineering and SQL & Python. Reqs. education, experience, or training in: data modeling techniques such as star-/snowflake-schema, de-normalized data modeling, and 3NF; data formats such as parquet, avro, delta, csv, and json; data processing techniques such as full-batch processing, time-based partitioning, distributed and real-time processing; data profiling; and, CI/CD and version control tools. May permit telecommuting. Apply at www.healthpartners.com/careers. Reference Job ID 10031. ABOUT US At HealthPartners we believe in the power of good – good deeds and good people working together. As part of our team, you’ll find an inclusive environment that encourages new ways of thinking, celebrates differences, and recognizes hard work. We’re a nonprofit, integrated health care organization, providing health insurance in six states and high-quality care at more than 90 locations, including hospitals and clinics in Minnesota and Wisconsin. We bring together research and education through HealthPartners Institute, training medical professionals across the region and conducting innovative research that improve lives around the world. At HealthPartners, everyone is welcome, included and valued. We’re working together to increase diversity and inclusion in our workplace, advance health equity in care and coverage, and partner with the community as advocates for change. Join us and become a partner for good, helping to improve the health and well-being of our patients, members and the communities we serve. We are an Equal Opportunity Employer and do not discriminate against any employee or applicant because of race, color, sex, age, national origin, religion, sexual orientation, gender identify, status as a veteran and basis of disability or any other federal, state or local protected class. JOB INFO Job Identification 100301 Posting Date 01/08/2024, 04:21 PM Locations 8170 33rd Ave S - Bloomington Work Schedule Monday-Friday; Core business hours Hours Per Week/FTE 40 hrs weekly / 1.0 FTE Job Shift Day Position Type Full-time regular Job Category Information Technology Department Health Informatics",Senior Data Engineer,"Bloomington, MN","JOB DESCRIPTION SENIOR DATA ENGINEER (Multiple Positions) – BLOOMINGTON, MN. Responsible for building, managing, and optimizing data pipelines that facilitate data movement in service of these goals by implementing and testing methods or build systems that improve data reliability and quality. Duties include: work with stakeholders, data scientists and analysts to frame problems, clean and integrate data, and determine best way to provision data on demand; collaborate with developers to design technology solutions that achieve measurable results at scale; help design and develop scalable, efficient data pipeline processes to handle data ingestion, cleansing, transformation, integration, and validation required to provide access to prepared data sets for analysts and data scientists; utilize development best practices including technical design reviews, implementing test plans, monitoring/alerting, peer code reviews, and documentation; collaborate with cross functional team to resolve data quality and operational issues and ensure timely delivery of products; incorporate core data management competencies including data governance, data security and data quality; participate in requirements gathering sessions to distill technical requirement from business requests; and, identify, design, and implement internal process improvements. Reqs. Master’s degree in Computer Science, Engineering, or closely related technical field, or, Bachelor’s degree in Computer Science, Engineering, or closely related technical field plus 2 years’ experience with hands-on data engineering and SQL & Python. Reqs. education, experience, or training in: data modeling techniques such as star-/snowflake-schema, de-normalized data modeling, and 3NF; data formats such as parquet, avro, delta, csv, and json; data processing techniques such as full-batch processing, time-based partitioning, distributed and real-time processing; data profiling; and, CI/CD and version control tools. May permit telecommuting. Apply at www.healthpartners.com/careers. Reference Job ID 10031. ABOUT US At HealthPartners we believe in the power of good – good deeds and good people working together. As part of our team, you’ll find an inclusive environment that encourages new ways of thinking, celebrates differences, and recognizes hard work. We’re a nonprofit, integrated health care organization, providing health insurance in six states and high-quality care at more than 90 locations, including hospitals and clinics in Minnesota and Wisconsin. We bring together research and education through HealthPartners Institute, training medical professionals across the region and conducting innovative research that improve lives around the world. At HealthPartners, everyone is welcome, included and valued. We’re working together to increase diversity and inclusion in our workplace, advance health equity in care and coverage, and partner with the community as advocates for change. Join us and become a partner for good, helping to improve the health and well-being of our patients, members and the communities we serve. We are an Equal Opportunity Employer and do not discriminate against any employee or applicant because of race, color, sex, age, national origin, religion, sexual orientation, gender identify, status as a veteran and basis of disability or any other federal, state or local protected class. JOB INFO Job Identification 100301 Posting Date 01/08/2024, 04:21 PM Locations 8170 33rd Ave S - Bloomington Work Schedule Monday-Friday; Core business hours Hours Per Week/FTE 40 hrs weekly / 1.0 FTE Job Shift Day Position Type Full-time regular Job Category Information Technology Department Health Informatics",Competitive,2024-01-14
Amex,"",Senior Data Engineer,Kansas,"","$110,000 - $190,000 a year",2024-01-14
